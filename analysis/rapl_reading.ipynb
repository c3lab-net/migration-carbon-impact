{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ccbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, sys, argparse, glob\n",
    "import csv\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import *\n",
    "from matplotlib_helper import *\n",
    "\n",
    "# plt.rcParams['svg.fonttype'] = 'none'\n",
    "#\n",
    "# import matplotlib_inline.backend_inline\n",
    "# matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6608242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeprefix(self: str, prefix: str, /) -> str:\n",
    "    if self.startswith(prefix):\n",
    "        return self[len(prefix):]\n",
    "    else:\n",
    "        return self[:]\n",
    "\n",
    "def removesuffix(self: str, suffix: str, /) -> str:\n",
    "    # suffix='' should not call self[:-0].\n",
    "    if suffix and self.endswith(suffix):\n",
    "        return self[:-len(suffix)]\n",
    "    else:\n",
    "        return self[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3077c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_prefix(l):\n",
    "    \"Given a list of pathnames, returns the longest common leading component\"\n",
    "    if not l: return ''\n",
    "    s1 = min(l)\n",
    "    s2 = max(l)\n",
    "    for i, c in enumerate(s1):\n",
    "        if c != s2[i]:\n",
    "            return s1[:i]\n",
    "    return s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cdf_array(array, label, include_count = False, index=0, color=None):\n",
    "    x = sorted(array)\n",
    "    y = np.linspace(0., 1., len(array) + 1)[1:]\n",
    "    if include_count:\n",
    "        label += ' (%d)' % len(array)\n",
    "    if color is None:\n",
    "        color = get_next_color()\n",
    "    plt.plot(x, y, label=label, color=color, linestyle=get_linestyle(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def to_relative_timestamp(timestamp: datetime, l_timestamp: list[datetime]):\n",
    "    return (timestamp - l_timestamp[0]).total_seconds()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f883331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(data_array, plot_axis=None, timestamp_column_name='timestamp', prefix=None, use_relative_time=False, color=None, index=0):\n",
    "    x = [entry[timestamp_column_name] for entry in data_array]\n",
    "    if use_relative_time:\n",
    "        x = [to_relative_timestamp(t, x) for t in x]\n",
    "    data_keys = []\n",
    "    for key in data_array[0].keys():\n",
    "        if key == timestamp_column_name:\n",
    "            continue\n",
    "        data_keys.append(key)\n",
    "    lines = []\n",
    "    for key in data_keys:\n",
    "        data_series = [entry[key] for entry in data_array]\n",
    "        label = ('%s - ' % prefix if prefix else '') + key\n",
    "        if plot_axis is None:\n",
    "            plot_axis = plt.gca()\n",
    "        line = plot_axis.plot(x, data_series, color=color, linestyle=get_linestyle(index), label=label)\n",
    "        index += 1\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_interval_boundary(l_timestamps, interval, label, plot_axis=None, use_relative_time=False):\n",
    "    assert len(interval) == 2, f\"Interval {interval} must have exactly two elements.\"\n",
    "    if plot_axis is None:\n",
    "        plot_axis = plt.gca()\n",
    "    xs = []\n",
    "    for timestamp_index in interval:\n",
    "        timestamp = l_timestamps[timestamp_index]\n",
    "        x = to_relative_timestamp(timestamp, l_timestamps) if use_relative_time else timestamp\n",
    "        xs.append(x)\n",
    "        plot_axis.vlines(x, 0, 100, color='black', linestyle='dashed')\n",
    "    average_timestamp = xs[0] + (xs[1] - xs[0]) / 2\n",
    "    plot_axis.text(average_timestamp, 100, label, ha='center', va='bottom')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rapl_data(rapl_log_file):\n",
    "    data_array = []\n",
    "    with open(rapl_log_file, 'r') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        column_names = csv_reader.fieldnames\n",
    "        required_columns = set(['timestamp', 'total_intel_energy', 'total_cpu_energy', 'total_dram_energy'])\n",
    "        assert [required_column in column_names for required_column in required_columns]\n",
    "        for row in csv_reader:\n",
    "            timestamp = datetime.fromisoformat(row['timestamp'])\n",
    "            total_intel_energy = float(row['total_intel_energy'])\n",
    "            total_cpu_energy = float(row['total_cpu_energy'])\n",
    "            total_dram_energy = float(row['total_dram_energy'])\n",
    "            data_array.append({\n",
    "                'timestamp': timestamp,\n",
    "                'total_intel_energy': total_intel_energy,\n",
    "                'total_cpu_energy': total_cpu_energy,\n",
    "                'total_dram_energy': total_dram_energy,\n",
    "            })\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_cpu_mem_usage_data(usage_log_file):\n",
    "    data_array = []\n",
    "    with open(usage_log_file, 'r') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        column_names = csv_reader.fieldnames\n",
    "        required_columns = set(['timestamp', 'cpu-user', 'cpu-kernel', 'cpu-idle', 'mem-used', 'mem-free'])\n",
    "        assert [required_column in column_names for required_column in required_columns]\n",
    "        for row in csv_reader:\n",
    "            timestamp = datetime.fromisoformat(row['timestamp'])\n",
    "            cpu_user = float(row['cpu-user'])\n",
    "            cpu_kernel = float(row['cpu-kernel'])\n",
    "            cpu_idle = float(row['cpu-idle'])\n",
    "            mem_used = float(row['mem-used'])\n",
    "            mem_free = float(row['mem-free'])\n",
    "            if mem_used + mem_free <= 0:\n",
    "                print(row, file=sys.stderr)\n",
    "            data_array.append({\n",
    "                'timestamp': timestamp,\n",
    "                'cpu': cpu_user + cpu_kernel,\n",
    "                'mem': mem_used / (mem_used + mem_free),\n",
    "                # 'cpu-user': cpu_user,\n",
    "                # 'cpu-kernel': cpu_kernel,\n",
    "                # 'cpu-idle': cpu_idle,\n",
    "                # 'mem-used': mem_used,\n",
    "                # 'mem-free': mem_free,\n",
    "            })\n",
    "    return data_array"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def auto_detect_log_files(dirpath):\n",
    "    dirpath = os.path.expanduser(dirpath)\n",
    "    RAPL_LOGFILE_SUFFIX = \".rapl.csv\"\n",
    "    USAGE_LOGFILE_SUFFIX = \".usage.csv\"\n",
    "    rapl_log_files = sorted(glob.glob(os.path.join(dirpath, '*' + RAPL_LOGFILE_SUFFIX)))\n",
    "    usage_log_files = sorted(glob.glob(os.path.join(dirpath, '*' + USAGE_LOGFILE_SUFFIX)))\n",
    "    assert len(rapl_log_files) == len(usage_log_files)\n",
    "    assert [removesuffix(filename, RAPL_LOGFILE_SUFFIX) for filename in rapl_log_files] == [removesuffix(filename, USAGE_LOGFILE_SUFFIX) for filename in usage_log_files]\n",
    "    return list(zip(rapl_log_files, usage_log_files))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_energy_delta_into_stages(l_delta_power):\n",
    "    \"\"\"Parse array into ranges of starting/ending stable power periods and middle varying power period.\"\"\"\n",
    "    POWER_DIFF_THRESHOLD = 1.\n",
    "    N_CONSECUTIVE = 10\n",
    "    SLIDING_WINDOW_SIZE = N_CONSECUTIVE - 1\n",
    "    l_is_stable = np.convolve(np.abs(l_delta_power) < POWER_DIFF_THRESHOLD, v=np.ones(N_CONSECUTIVE), mode='valid') == N_CONSECUTIVE\n",
    "    l_stable_groups = []\n",
    "    TupleIsStableAndDuration = namedtuple('TupleIsStableAndDuration', ['is_stable', 'duration'])\n",
    "    for is_stable, group in itertools.groupby(l_is_stable):\n",
    "        duration = sum(1 for _ in group)\n",
    "        l_stable_groups.append(TupleIsStableAndDuration(is_stable, duration))\n",
    "    if len(l_stable_groups) < 3:\n",
    "        raise ValueError(\"Not enough groups for stable -> varying -> stable power readings\")\n",
    "\n",
    "    # alternating T/Ts due to groupby: (_, F) (optional), (a, T), (b, F), ..., (y, T), (z, F) (optional)\n",
    "    # First and last unstable groups are discarded, so the first and last stable groups are true idle periods,\n",
    "    #   and all the middle section is considered running period.\n",
    "    index_stable_groups = np.where(np.array([t.is_stable for t in l_stable_groups]))[0]\n",
    "    index_first_stable_group = index_stable_groups[0]\n",
    "    index_last_stable_group = index_stable_groups[-1]\n",
    "    assert index_first_stable_group in [0, 1]\n",
    "    assert index_last_stable_group in [len(l_stable_groups) - 2, len(l_stable_groups) - 1]\n",
    "\n",
    "    # calculate the [start_index, end_index) intervals for first and last stable group as idle start and idle end period\n",
    "    first_stable_group_start = l_stable_groups[0].duration if not l_stable_groups[0].is_stable else 0\n",
    "    first_stable_group_end = first_stable_group_start + (l_stable_groups[index_first_stable_group].duration + SLIDING_WINDOW_SIZE)\n",
    "    last_stable_group_end = len(l_delta_power) - (l_stable_groups[-1].duration if not l_stable_groups[-1].is_stable else 0)\n",
    "    last_stable_group_start = last_stable_group_end - (l_stable_groups[index_last_stable_group].duration + SLIDING_WINDOW_SIZE)\n",
    "\n",
    "    # Properly formulate the start/end idle intervals and everything in between as running interval\n",
    "    idle_start_range = [first_stable_group_start, first_stable_group_end]\n",
    "    running_range = [first_stable_group_end, last_stable_group_start]\n",
    "    idle_end_range = [last_stable_group_start, last_stable_group_end]\n",
    "    return idle_start_range, running_range, idle_end_range"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data_array is a list of dicts, each of which has 'timestamp', 'total_intel_energy', ...\n",
    "# it's already sorted by timestamp\n",
    "def get_energy_stats(data_array):\n",
    "    assert len(data_array) > 2, \"Time series is too short\"\n",
    "    l_timestamp = np.array([entry['timestamp'] for entry in data_array])\n",
    "    delta_timestamps = [int(delta.total_seconds()) for delta in np.diff(l_timestamp, n=1)]\n",
    "    sample_interval_s = delta_timestamps[0]\n",
    "    # print('Sample intervals:', delta_timestamps)\n",
    "    assert all(delta == sample_interval_s for delta in delta_timestamps)\n",
    "\n",
    "    # detect idle power draw via diff among consecutive readings\n",
    "    l_power = np.array([entry['total_intel_energy']/sample_interval_s for entry in data_array])\n",
    "    # print(l_power)\n",
    "    l_delta_power = np.diff(l_power, n=1)\n",
    "    # print(l_delta_power)\n",
    "    assert len(l_power) == len(l_delta_power) + 1\n",
    "\n",
    "    # Break down readings into three stages in [start_index, end_index) intervals\n",
    "    idle_start_range, running_range, idle_end_range = parse_energy_delta_into_stages(l_delta_power)\n",
    "    # print(idle_start_range, running_range, idle_end_range)\n",
    "    l_power_idle_before = l_power[idle_start_range[0]:idle_start_range[1]]\n",
    "    l_power_workload = l_power[running_range[0]:running_range[1]]\n",
    "    l_power_idle_after = l_power[idle_end_range[0]:idle_end_range[1]]\n",
    "\n",
    "    avg_power_idle_before = np.average(l_power_idle_before)\n",
    "    std_power_idle_before = np.std(l_power_idle_before)\n",
    "    avg_power_idle_after = np.average(l_power_idle_after)\n",
    "    std_power_idle_after = np.std(l_power_idle_after)\n",
    "    # print(avg_power_idle_before, std_power_idle_before, avg_power_idle_after, std_power_idle_after)\n",
    "\n",
    "    IDLE_POWER_STD_THRESHOLD = 0.05\n",
    "    assert std_power_idle_before / avg_power_idle_before < IDLE_POWER_STD_THRESHOLD, \\\n",
    "        \"Idle power std is too high (%.2f+/-%.2f)\" % (avg_power_idle_before, std_power_idle_before)\n",
    "    assert std_power_idle_after / avg_power_idle_after < IDLE_POWER_STD_THRESHOLD, \\\n",
    "        \"Idle power std is too high (%.2f+/-%.2f)\" % (avg_power_idle_after, std_power_idle_after)\n",
    "\n",
    "    # \"Idle power before/after difference is too high\"\n",
    "    avg_power_idle = np.average([avg_power_idle_before, avg_power_idle_after])\n",
    "#     print('Workload duration: %ds' % len(l_power_workload) * sample_interval_s)\n",
    "#     print('Idle power: %.fW' % (avg_power_idle / sample_interval_s))\n",
    "    running_duration = len(l_power_workload) * sample_interval_s\n",
    "    return {\n",
    "        'duration': running_duration,\n",
    "        'timestamps': l_timestamp,\n",
    "        'idle_start_range': idle_start_range,\n",
    "        'running_range': running_range,\n",
    "        'idle_end_range': idle_end_range,\n",
    "        'total_energy': np.sum(l_power_workload),\n",
    "        'delta_energy': np.sum(l_power_workload) - running_duration * avg_power_idle,\n",
    "        'sample_interval_s': sample_interval_s,\n",
    "        'idle_power': avg_power_idle,\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.realpath('../data/')\n",
    "\n",
    "l_sample_interval_s = []\n",
    "# prepend these with \"video-transcoding/logs/\"\n",
    "log_files_pairs = [\n",
    "    # ('combined/ffmpeg.rapl.csv', 'combined/ffmpeg.usage.csv'),\n",
    "    # ('combined/scp.src.rapl.csv', 'combined/scp.src.usage.csv'),\n",
    "    # ('combined/scp.dst.rapl.csv', 'combined/scp.dst.usage.csv'),\n",
    "    # ('mbw/mbw.2GB.rapl.csv', 'mbw/mbw.2GB.usage.csv'),\n",
    "    # ('mbw/mbw.32GB.rapl.csv', 'mbw/mbw.32GB.usage.csv'),\n",
    "    # ('stress.cpu=40.timeout=15/stress.cpu=40.timeout=15.rapl.csv', 'stress.cpu=40.timeout=15/stress.cpu=40.timeout=15.usage.csv'),\n",
    "    # 'ffmpeg-Rain.csv'\n",
    "    # 'ffmpeg-Rain-10x.csv',\n",
    "    # 'ffmpeg-Rain.data-copy.1G.csv',\n",
    "    # 'ffmpeg.youtube-wnhvanMdx4s.720p.csv',\n",
    "    # 'ffmpeg-Rain.data-copy.100G.csv',\n",
    "    # 'spark-wordcount-short.csv',\n",
    "    # 'spark-wordcount-long.csv',\n",
    "    # 'spark-wordcount-long.data-copy.1G.csv',\n",
    "    # 'spark-wordcount-long.data-copy.100G.csv',\n",
    "]\n",
    "wildcard_dir_names = [\n",
    "    # 'video-transcoding/logs/stress.smt=on.cpu=*.timeout=60',\n",
    "    # 'video-transcoding/logs/stress.smt=off.cpu=*.timeout=60',\n",
    "    # 'video-transcoding/logs/mbw.*',\n",
    "    # 'video-transcoding/logs/ffmpeg.youtube-wnhvanMdx4s.720p.grayscale',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.1x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.2x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.4x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.8x',\n",
    "    # 'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.9x',\n",
    "    # 'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.10x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.16x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.32x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.64x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.128x',\n",
    "    # 'code-compilation/compile.linux.ramdisk',\n",
    "    # 'code-compilation/compile.linux.ssd',\n",
    "    # 'file-compression/gzip.linux.ramdisk',\n",
    "    # 'file-compression/gzip.linux.ssd',\n",
    "    # 'file-compression/bzip.linux.ramdisk',\n",
    "    # 'file-compression/bzip.linux.ssd',\n",
    "    # 'jupyter-notebook/rawdata'\n",
    "]\n",
    "nested_array_of_file_pairs = [auto_detect_log_files(os.path.join(ROOT_DIR, wildcard_dir_name)) for wildcard_dir_name in wildcard_dir_names]\n",
    "log_files_pairs = list(itertools.chain.from_iterable(nested_array_of_file_pairs))\n",
    "l_parallelism = np.array([1, 2, 4, 8, 16, 32, 64, 128])\n",
    "l_runtime = []\n",
    "l_total_energy = []\n",
    "l_delta_energy = []\n",
    "for (rapl_log_file, cpu_mem_usage_log_file) in log_files_pairs:\n",
    "    name = common_prefix([rapl_log_file, cpu_mem_usage_log_file])\n",
    "    name = name.split('/')[-1].rstrip('.')\n",
    "    print(\"Workload: %s\" % name)\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    plot_lines = []\n",
    "    energy_data_array = get_rapl_data(os.path.join(ROOT_DIR, rapl_log_file))\n",
    "    plot_lines += plot_timeseries(energy_data_array, plot_axis=ax1, prefix=\"RAPL\", use_relative_time=False, color='orange')\n",
    "    energy_stats = get_energy_stats(energy_data_array)\n",
    "    l_sample_interval_s.append(energy_stats['sample_interval_s'])\n",
    "    # print('Workload: %s' % rapl_log_file)\n",
    "    stats_duration = energy_stats['duration']\n",
    "    stats_total_energy = energy_stats['total_energy']\n",
    "    stats_delta_energy = energy_stats['delta_energy']\n",
    "    print('Duration: %.fs' % stats_duration)\n",
    "    print('Total energy: %.fJ' % stats_total_energy)\n",
    "    print('Delta energy: %.fJ' % stats_delta_energy)\n",
    "    energy_stats_text = f'{stats_duration:.0f}s {stats_total_energy:.0f}J / {stats_delta_energy:.0f}J'\n",
    "    l_runtime.append(energy_stats['duration'])\n",
    "    l_total_energy.append(energy_stats['total_energy'])\n",
    "    l_delta_energy.append(energy_stats['delta_energy'])\n",
    "    cpu_mem_usage_data_array = get_cpu_mem_usage_data(os.path.join(ROOT_DIR, cpu_mem_usage_log_file))\n",
    "    plot_lines += plot_timeseries(cpu_mem_usage_data_array, plot_axis=ax2, prefix=\"Usage\", use_relative_time=False, color='blue', index=0)\n",
    "    # TODO: plot interval boundaries on timeseries\n",
    "    l_timestamps = energy_stats['timestamps']\n",
    "    plot_interval_boundary(l_timestamps, energy_stats['idle_start_range'], 'start\\nidle', plot_axis=ax2, use_relative_time=False)\n",
    "    plot_interval_boundary(l_timestamps, energy_stats['running_range'], f'running\\n{energy_stats_text}', plot_axis=ax2, use_relative_time=False)\n",
    "    plot_interval_boundary(l_timestamps, energy_stats['idle_end_range'], 'end\\nidle', plot_axis=ax2, use_relative_time=False)\n",
    "    # plot_labels = [line.get_label() for line in plot_lines]\n",
    "    # plot_labels = ax1.lines + ax2.lines\n",
    "    # ax1.legend(plot_lines, plot_labels)\n",
    "    # plt.xlim(0, 20)\n",
    "    # plt.ylim(105, 115)\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Power (W)', color='orange')\n",
    "    ax2.set_ylabel('Utilization (%)', color='blue')\n",
    "    ax1.set_ylim(0, 300)\n",
    "    ax2.set_ylim(0, 120)\n",
    "    ax1.tick_params(axis='x', rotation=15)\n",
    "    ax1.grid()\n",
    "    # plt.locator_params(axis='y', nbins=5)\n",
    "    fig.legend(loc='center left', bbox_to_anchor=(1, 0.52))\n",
    "    # ax2.legend(loc='lower center')\n",
    "    plt.title('Workload: %s' % name)\n",
    "    plt.savefig('%s.svg' % name, bbox_inches='tight')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b19922",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_total_energy_per_job = l_total_energy / l_parallelism\n",
    "l_delta_energy_per_job = l_delta_energy / l_parallelism\n",
    "fig = plt.figure()\n",
    "ax1 = fig.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(l_parallelism, l_runtime, marker='.', label='Runtime', color='black', linestyle=get_linestyle(0))\n",
    "ax2.plot(l_parallelism, l_total_energy_per_job, marker='.', label='Total energy/job', color='blue', linestyle=get_linestyle(1))\n",
    "ax2.plot(l_parallelism, l_delta_energy_per_job, marker='.', label='Delta energy/job', color='blue', linestyle=get_linestyle(2))\n",
    "\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('# of parallel jobs')\n",
    "ax1.set_ylabel('Runtime (s)', color='black')\n",
    "ax2.set_ylabel('Energy/job (J)', color='blue')\n",
    "ax1.set_ylim(0, 3000)\n",
    "ax2.set_ylim(0, None)\n",
    "ax1.set_xticks(l_parallelism)\n",
    "ax1.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "# ax1.tick_params(axis='x', rotation=15)\n",
    "ax1.grid()\n",
    "# plt.locator_params(axis='y', nbins=5)\n",
    "fig.legend(loc='center left', bbox_to_anchor=(1, 0.52))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c686f18b94c7ac93143553ca22fc47dddb45e62f4fa825973de80c268de3ec5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
