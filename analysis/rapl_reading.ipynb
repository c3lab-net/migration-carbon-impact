{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ccbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, sys, argparse, glob\n",
    "import csv\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import *\n",
    "from matplotlib_helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6608242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeprefix(self: str, prefix: str, /) -> str:\n",
    "    if self.startswith(prefix):\n",
    "        return self[len(prefix):]\n",
    "    else:\n",
    "        return self[:]\n",
    "\n",
    "def removesuffix(self: str, suffix: str, /) -> str:\n",
    "    # suffix='' should not call self[:-0].\n",
    "    if suffix and self.endswith(suffix):\n",
    "        return self[:-len(suffix)]\n",
    "    else:\n",
    "        return self[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3077c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_prefix(l):\n",
    "    \"Given a list of pathnames, returns the longest common leading component\"\n",
    "    if not l: return ''\n",
    "    s1 = min(l)\n",
    "    s2 = max(l)\n",
    "    for i, c in enumerate(s1):\n",
    "        if c != s2[i]:\n",
    "            return s1[:i]\n",
    "    return s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03c2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cdf_array(array, label, include_count = False, index=0, color=None):\n",
    "    x = sorted(array)\n",
    "    y = np.linspace(0., 1., len(array) + 1)[1:]\n",
    "    if include_count:\n",
    "        label += ' (%d)' % len(array)\n",
    "    if color is None:\n",
    "        color = get_next_color()\n",
    "    plt.plot(x, y, label=label, color=color, linestyle=get_linestyle(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f883331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries(data_array, plot_axis=None, timestamp_column_name='timestamp', prefix=None, use_relative_time=False, color=None, index=0):\n",
    "    x = [entry[timestamp_column_name] for entry in data_array]\n",
    "    if use_relative_time:\n",
    "        start_time = x[0]\n",
    "        x = [(t - start_time).total_seconds() for t in x]\n",
    "    data_keys = []\n",
    "    for key in data_array[0].keys():\n",
    "        if key == timestamp_column_name:\n",
    "            continue\n",
    "        data_keys.append(key)\n",
    "    lines = []\n",
    "    for key in data_keys:\n",
    "        data_series = [entry[key] for entry in data_array]\n",
    "        label = ('%s - ' % prefix if prefix else '') + key\n",
    "        if plot_axis is None:\n",
    "            plot_axis = plt.gca()\n",
    "        line = plot_axis.plot(x, data_series, color=color, linestyle=get_linestyle(index), label=label)\n",
    "        index += 1\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rapl_data(rapl_log_file):\n",
    "    data_array = []\n",
    "    with open(rapl_log_file, 'r') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        column_names = csv_reader.fieldnames\n",
    "        required_columns = set(['timestamp', 'total_intel_energy', 'total_cpu_energy', 'total_dram_energy'])\n",
    "        assert [required_column in column_names for required_column in required_columns]\n",
    "        for row in csv_reader:\n",
    "            timestamp = datetime.fromisoformat(row['timestamp'])\n",
    "            total_intel_energy = float(row['total_intel_energy'])\n",
    "            total_cpu_energy = float(row['total_cpu_energy'])\n",
    "            total_dram_energy = float(row['total_dram_energy'])\n",
    "            data_array.append({\n",
    "                'timestamp': timestamp,\n",
    "                'total_intel_energy': total_intel_energy,\n",
    "                'total_cpu_energy': total_cpu_energy,\n",
    "                'total_dram_energy': total_dram_energy,\n",
    "            })\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpu_mem_usage_data(usage_log_file):\n",
    "    data_array = []\n",
    "    with open(usage_log_file, 'r') as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        column_names = csv_reader.fieldnames\n",
    "        required_columns = set(['timestamp', 'cpu-user', 'cpu-kernel', 'cpu-idle', 'mem-used', 'mem-free'])\n",
    "        assert [required_column in column_names for required_column in required_columns]\n",
    "        for row in csv_reader:\n",
    "            timestamp = datetime.fromisoformat(row['timestamp'])\n",
    "            cpu_user = float(row['cpu-user'])\n",
    "            cpu_kernel = float(row['cpu-kernel'])\n",
    "            cpu_idle = float(row['cpu-idle'])\n",
    "            mem_used = float(row['mem-used'])\n",
    "            mem_free = float(row['mem-free'])\n",
    "            data_array.append({\n",
    "                'timestamp': timestamp,\n",
    "                'cpu': cpu_user + cpu_kernel,\n",
    "                'mem': mem_used / (mem_used + mem_free),\n",
    "                # 'cpu-user': cpu_user,\n",
    "                # 'cpu-kernel': cpu_kernel,\n",
    "                # 'cpu-idle': cpu_idle,\n",
    "                # 'mem-used': mem_used,\n",
    "                # 'mem-free': mem_free,\n",
    "            })\n",
    "    return data_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fd3ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_array is a list of dicts, each of which has 'timestamp', 'total_intel_energy', ...\n",
    "# it's already sorted by timestamp\n",
    "def get_energy_stats(data_array):\n",
    "    assert len(data_array) > 2, \"Time series is too short\"\n",
    "    l_timestamp = np.array([entry['timestamp'] for entry in data_array])\n",
    "    delta_timestamps = [int(delta.total_seconds()) for delta in np.diff(l_timestamp, n=1)]\n",
    "    sample_interval_s = delta_timestamps[0]\n",
    "    assert all(delta == sample_interval_s for delta in delta_timestamps)\n",
    "#     print('Sample interval: %ds' % sample_interval_s)\n",
    "\n",
    "    # detect idle power draw\n",
    "    l_power = np.array([entry['total_intel_energy']/sample_interval_s for entry in data_array])\n",
    "    # print(l_power)\n",
    "    delta_power = np.diff(l_power, n=1)\n",
    "    # print(delta_power)\n",
    "    assert len(l_power) == len(delta_power) + 1\n",
    "    POWER_DIFF_THRESHOLD = 1\n",
    "    IDLE_POWER_STD_THRESHOLD = 0.01\n",
    "    index_workload_start = np.argmax(delta_power > POWER_DIFF_THRESHOLD)\n",
    "    index_workload_end = len(delta_power) - np.argmax(delta_power[::-1] < -POWER_DIFF_THRESHOLD)\n",
    "    l_power_idle_before = l_power[:index_workload_start]\n",
    "    l_power_workload = l_power[index_workload_start:index_workload_end]\n",
    "    l_power_idle_after = l_power[index_workload_end:]\n",
    "    avg_power_idle_before = np.average(l_power_idle_before)\n",
    "    std_power_idle_before = np.std(l_power_idle_before)\n",
    "    avg_power_idle_after = np.average(l_power_idle_after)\n",
    "    std_power_idle_after = np.std(l_power_idle_after)\n",
    "    # print(avg_power_idle_before, std_power_idle_before, avg_power_idle_after, std_power_idle_after)\n",
    "\n",
    "    assert std_power_idle_before / avg_power_idle_before < IDLE_POWER_STD_THRESHOLD, \"Idle power std is too high\"\n",
    "    assert std_power_idle_after / avg_power_idle_after < IDLE_POWER_STD_THRESHOLD, \"Idle power std is too high\"\n",
    "    \n",
    "    # \"Idle power before/after difference is too high\"\n",
    "    avg_power_idle = np.average([avg_power_idle_before, avg_power_idle_after])\n",
    "    if np.abs(avg_power_idle_before - avg_power_idle_after) > POWER_DIFF_THRESHOLD:\n",
    "        avg_power_idle = avg_power_idle_before\n",
    "\n",
    "#     print('Workload duration: %ds' % len(l_power_workload) * sample_interval_s)\n",
    "#     print('Idle power: %.fW' % (avg_power_idle / sample_interval_s))\n",
    "    return {\n",
    "        'duration': len(l_power_workload) * sample_interval_s,\n",
    "        'start_index': index_workload_start,\n",
    "        'total_energy': np.sum(l_power_workload),\n",
    "        'delta_energy': np.sum(l_power_workload) - len(l_power_workload) * avg_power_idle,\n",
    "        'sample_interval_s': sample_interval_s,\n",
    "        'idle_power': avg_power_idle,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea57d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_detect_log_files(dirpath):\n",
    "    dirpath = os.path.expanduser(dirpath)\n",
    "    RAPL_LOGFILE_SUFFIX = \".rapl.csv\"\n",
    "    USAGE_LOGFILE_SUFFIX = \".usage.csv\"\n",
    "    rapl_log_files = sorted(glob.glob(os.path.join(dirpath, '*' + RAPL_LOGFILE_SUFFIX)))\n",
    "    usage_log_files = sorted(glob.glob(os.path.join(dirpath, '*' + USAGE_LOGFILE_SUFFIX)))\n",
    "    assert len(rapl_log_files) == len(usage_log_files)\n",
    "    assert [removesuffix(filename, RAPL_LOGFILE_SUFFIX) for filename in rapl_log_files] == [removesuffix(filename, USAGE_LOGFILE_SUFFIX) for filename in usage_log_files]\n",
    "    return list(zip(rapl_log_files, usage_log_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.realpath('../data/')\n",
    "\n",
    "l_sample_interval_s = []\n",
    "# prepend these with \"video-transcoding/logs/\"\n",
    "log_files_pairs = [\n",
    "    # ('combined/ffmpeg.rapl.csv', 'combined/ffmpeg.usage.csv'),\n",
    "    # ('combined/scp.src.rapl.csv', 'combined/scp.src.usage.csv'),\n",
    "    # ('combined/scp.dst.rapl.csv', 'combined/scp.dst.usage.csv'),\n",
    "    # ('mbw/mbw.2GB.rapl.csv', 'mbw/mbw.2GB.usage.csv'),\n",
    "    # ('mbw/mbw.32GB.rapl.csv', 'mbw/mbw.32GB.usage.csv'),\n",
    "    # ('stress.cpu=40.timeout=15/stress.cpu=40.timeout=15.rapl.csv', 'stress.cpu=40.timeout=15/stress.cpu=40.timeout=15.usage.csv'),\n",
    "    # 'ffmpeg-Rain.csv'\n",
    "    # 'ffmpeg-Rain-10x.csv',\n",
    "    # 'ffmpeg-Rain.data-copy.1G.csv',\n",
    "    # 'ffmpeg.youtube-wnhvanMdx4s.720p.csv',\n",
    "    # 'ffmpeg-Rain.data-copy.100G.csv',\n",
    "    # 'spark-wordcount-short.csv',\n",
    "    # 'spark-wordcount-long.csv',\n",
    "    # 'spark-wordcount-long.data-copy.1G.csv',\n",
    "    # 'spark-wordcount-long.data-copy.100G.csv',\n",
    "]\n",
    "wildcard_dir_names = [\n",
    "    # 'video-transcoding/logs/stress.smt=on.cpu=*.timeout=60',\n",
    "    # 'video-transcoding/logs/stress.smt=off.cpu=*.timeout=60',\n",
    "    # 'video-transcoding/logs/mbw.*',\n",
    "    # 'video-transcoding/logs/ffmpeg.youtube-wnhvanMdx4s.720p.grayscale',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.1x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.2x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.4x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.8x',\n",
    "    # 'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.9x',\n",
    "    # 'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.10x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.16x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.32x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.64x',\n",
    "    'video-transcoding/logs/ffmpeg.resize.4k-1080p.copyrighted-A66C0008.128x',\n",
    "    # 'code-compilation/compile.linux.ramdisk',\n",
    "    # 'code-compilation/compile.linux.ssd',\n",
    "    # 'file-compression/gzip.linux.ramdisk',\n",
    "    # 'file-compression/gzip.linux.ssd',\n",
    "    # 'file-compression/bzip.linux.ramdisk',\n",
    "    # 'file-compression/bzip.linux.ssd',\n",
    "    # 'jupyter-notebook/rawdata'\n",
    "]\n",
    "nested_array_of_file_pairs = [auto_detect_log_files(os.path.join(ROOT_DIR, wildcard_dir_name)) for wildcard_dir_name in wildcard_dir_names]\n",
    "log_files_pairs = list(itertools.chain.from_iterable(nested_array_of_file_pairs))\n",
    "l_parallelism = np.array([1, 2, 4, 8, 16, 32, 64, 128])\n",
    "l_runtime = []\n",
    "l_total_energy = []\n",
    "l_delta_energy = []\n",
    "for (rapl_log_file, cpu_mem_usage_log_file) in log_files_pairs:\n",
    "    name = common_prefix([rapl_log_file, cpu_mem_usage_log_file])\n",
    "    name = name.split('/')[-1].rstrip('.')\n",
    "    print(\"Workload: %s\" % name)\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    plot_lines = []\n",
    "    DISCARD_NUM_DATAPOINTS = 5\n",
    "    energy_data_array = get_rapl_data(os.path.join(ROOT_DIR, rapl_log_file))[DISCARD_NUM_DATAPOINTS:]\n",
    "    plot_lines += plot_timeseries(energy_data_array, plot_axis=ax1, prefix=\"RAPL\", use_relative_time=False, color='orange')\n",
    "    energy_stats = get_energy_stats(energy_data_array)\n",
    "    l_sample_interval_s.append(energy_stats['sample_interval_s'])\n",
    "    # print('Workload: %s' % rapl_log_file)\n",
    "    print('Duration: %.fs' % energy_stats['duration'])\n",
    "    print('Total energy: %.fJ' % energy_stats['total_energy'])\n",
    "    print('Delta energy: %.fJ' % energy_stats['delta_energy'])\n",
    "    l_runtime.append(energy_stats['duration'])\n",
    "    l_total_energy.append(energy_stats['total_energy'])\n",
    "    l_delta_energy.append(energy_stats['delta_energy'])\n",
    "    cpu_mem_usage_data_array = get_cpu_mem_usage_data(os.path.join(ROOT_DIR, cpu_mem_usage_log_file))[DISCARD_NUM_DATAPOINTS:]\n",
    "    plot_lines += plot_timeseries(cpu_mem_usage_data_array, plot_axis=ax2, prefix=\"Usage\", use_relative_time=False, color='blue', index=0)\n",
    "    # plot_labels = [line.get_label() for line in plot_lines]\n",
    "    # plot_labels = ax1.lines + ax2.lines\n",
    "    # ax1.legend(plot_lines, plot_labels)\n",
    "    # plt.xlim(0, 20)\n",
    "    # plt.ylim(105, 115)\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Power (W)', color='orange')\n",
    "    ax2.set_ylabel('Utilization (%)', color='blue')\n",
    "    ax1.set_ylim(0, 300)\n",
    "    ax2.set_ylim(0, 120)\n",
    "    ax1.tick_params(axis='x', rotation=15)\n",
    "    ax1.grid()\n",
    "    # plt.locator_params(axis='y', nbins=5)\n",
    "    fig.legend(loc='center left', bbox_to_anchor=(1, 0.52))\n",
    "    # ax2.legend(loc='lower center')\n",
    "    plt.title('Workload: %s' % name)\n",
    "    plt.savefig('%s.png' % name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b19922",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_total_energy_per_job = l_total_energy / l_parallelism\n",
    "l_delta_energy_per_job = l_delta_energy / l_parallelism\n",
    "fig = plt.figure()\n",
    "ax1 = fig.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(l_parallelism, l_runtime, marker='.', label='Runtime', color='black', linestyle=get_linestyle(0))\n",
    "ax2.plot(l_parallelism, l_total_energy_per_job, marker='.', label='Total energy/job', color='blue', linestyle=get_linestyle(1))\n",
    "ax2.plot(l_parallelism, l_delta_energy_per_job, marker='.', label='Delta energy/job', color='blue', linestyle=get_linestyle(2))\n",
    "\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('# of parallel jobs')\n",
    "ax1.set_ylabel('Runtime (s)', color='black')\n",
    "ax2.set_ylabel('Energy/job (J)', color='blue')\n",
    "ax1.set_ylim(0, 3000)\n",
    "ax2.set_ylim(0, None)\n",
    "ax1.set_xticks(l_parallelism)\n",
    "ax1.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "# ax1.tick_params(axis='x', rotation=15)\n",
    "ax1.grid()\n",
    "# plt.locator_params(axis='y', nbins=5)\n",
    "fig.legend(loc='center left', bbox_to_anchor=(1, 0.52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e323b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c95a14b0b15b2dff0ac63cf46f19a415aa466114342573829c0ea35de1c1134a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py39')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
