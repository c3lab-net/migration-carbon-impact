{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import copy\n",
    "import random\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from matplotlib_helper import *\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "\n",
    "metadata = {\n",
    "    'emission_rates': {\n",
    "        'ylabel': 'gCO2/s',\n",
    "        'title': 'Instantaneous emission rates'\n",
    "    },\n",
    "    'emission_integral': {\n",
    "        'ylabel': 'gCO2',\n",
    "        'title': 'Emission integral over its duration'\n",
    "    },\n",
    "}\n",
    "\n",
    "d_timing_labels = {\n",
    "    \"input_transfer_start\": \"Input transfer\",\n",
    "    # \"input_transfer_start\": \"Start of input transfer\",\n",
    "    # \"input_transfer_end\": \"End of input transfer\",\n",
    "    \"compute_start\": \"Compute\",\n",
    "    # \"compute_start\": \"Start of compute\",\n",
    "    # \"compute_end\": \"End of compute\",\n",
    "    \"output_transfer_start\": \"Output transfer\",\n",
    "    # \"output_transfer_start\": \"Start of output transfer\",\n",
    "    # \"output_transfer_end\": \"End of output transfer\",\n",
    "}\n",
    "\n",
    "d_events = {\n",
    "    'input_transfer': {\n",
    "        'interval_keys': (\"input_transfer_start\", \"input_transfer_end\"),\n",
    "        'label': 'Input transfer',\n",
    "    },\n",
    "    'compute': {\n",
    "        'interval_keys': (\"compute_start\", \"compute_end\"),\n",
    "        'label': 'Compute',\n",
    "    },\n",
    "    'output_transfer': {\n",
    "        'interval_keys': (\"output_transfer_start\", \"output_transfer_end\"),\n",
    "        'label': 'Output transfer',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_value(data_details: dict, series_name: str):\n",
    "    max_value = 0\n",
    "    for region in data_details:\n",
    "        compute_data = data_details[region][series_name][\"compute\"]\n",
    "        transfer_data = data_details[region][series_name][\"transfer\"]\n",
    "        max_value = max(max_value, max(compute_data.values(), default=0), max(transfer_data.values(), default=0))\n",
    "    return max_value\n",
    "\n",
    "def resample_timeseries(df: pd.DataFrame, interval: str):\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "    df.set_index(\"Timestamp\", inplace=True)\n",
    "    df_resampled = df.resample(interval).ffill().reset_index()\n",
    "    return df_resampled\n",
    "\n",
    "def create_dataframe_for_plotting(timeseries: dict[str, float], min_start: datetime, max_end: datetime) -> pd.DataFrame:\n",
    "    \"\"\"Convert a time series data to a dataframe, while removing out of bound timestamps.\n",
    "    \n",
    "        Args:\n",
    "            timeseries: A dictionary of timestamp strings and values.\n",
    "            min_start: The minimum cutoff time for the timeseries.\n",
    "            max_end: The maximum cutoff time for the timeseries.\n",
    "    \"\"\"\n",
    "    timeseries_in_datatime = {datetime.fromisoformat(key): value for key, value in timeseries.items()}\n",
    "    df = pd.DataFrame(list(timeseries_in_datatime.items()), columns=[\"Timestamp\", \"Value\"])\n",
    "    if df.empty:\n",
    "        return df\n",
    "    resampled = resample_timeseries(df, \"30s\")\n",
    "    mask = (resampled[\"Timestamp\"] >= pd.to_datetime(min_start)) & (resampled[\"Timestamp\"] <= pd.to_datetime(max_end))\n",
    "    return resampled[mask]\n",
    "\n",
    "def add_timing(ax, name: str, time: pd.Timestamp, max_value: float, color: str):\n",
    "    if 'start' in name:\n",
    "        ax.vlines(time, ymin=0, ymax=max_value, color='gray', alpha=0.5, linestyles=\"solid\" if 'compute' in name else \"dashed\")\n",
    "        if 'input' in name:\n",
    "            ha = 'right'\n",
    "            rotation = -30\n",
    "        elif 'output' in name:\n",
    "            ha = 'left'\n",
    "            rotation = 30\n",
    "        else:\n",
    "            ha = 'center'\n",
    "            rotation = 0\n",
    "        ax.text(time, max_value, d_timing_labels[name], color=color, alpha=0.95, ha=ha, va=\"bottom\", rotation=rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "def plot_carbon_api_response(data: dict, show_events=True, show_transfer_breakdown = True, show_compute=True, show_transfer=True):\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "    d_region_colors = {}\n",
    "    for region in data['details']:\n",
    "        d_region_colors[region] = get_next_color()\n",
    "\n",
    "    # Extract emission integral data\n",
    "    for (ax, series_name) in zip([axes], [\"emission_rates\", \"emission_integral\"]):\n",
    "        if series_name == 'emission_integral':\n",
    "            continue\n",
    "        # # Get max value for y-axis\n",
    "        # max_y_value = get_max_value(data['details'], series_name)\n",
    "\n",
    "        for region in data['details']:\n",
    "            # if region == 'Azure:eastus':\n",
    "            #     continue\n",
    "            print(f\"Plotting {region} - {series_name}\")\n",
    "            compute_data = data[\"details\"][region][series_name][\"compute\"]\n",
    "            transfer_data = data[\"details\"][region][series_name][\"transfer\"]\n",
    "            transfer_network_data = data[\"details\"][region][series_name][\"transfer.network\"]\n",
    "            transfer_endpoint_data = data[\"details\"][region][series_name][\"transfer.endpoint\"]\n",
    "            timings = data[\"details\"][region]['timings'][0] # Assume single occurence per job\n",
    "            min_start = datetime.fromisoformat(timings['min_start'])\n",
    "            max_end = datetime.fromisoformat(timings['max_end'])\n",
    "            carbon_emissions_compute = data['raw-scores'][region]['carbon-emission-from-compute']\n",
    "            carbon_emissions_transfer = data['raw-scores'][region]['carbon-emission-from-migration']\n",
    "\n",
    "            # Convert timestamp strings to datetime objects\n",
    "            compute_df = create_dataframe_for_plotting(compute_data, min_start, max_end)\n",
    "            transfer_df = create_dataframe_for_plotting(transfer_data, min_start, max_end)\n",
    "            transfer_network_df = create_dataframe_for_plotting(transfer_network_data, min_start, max_end)\n",
    "            transfer_endpoint_df = create_dataframe_for_plotting(transfer_endpoint_data, min_start, max_end)\n",
    "\n",
    "            # Plot timeseries data as step functions\n",
    "            color = d_region_colors[region]\n",
    "            if show_compute:\n",
    "                compute_time = pd.to_timedelta(timings['compute_duration']).floor('s').to_pytimedelta()\n",
    "                label_compute = f\"{region} - Compute ({carbon_emissions_compute:.3f} gCO2 over {compute_time})\"\n",
    "                ax.step(compute_df[\"Timestamp\"], compute_df[\"Value\"], label=label_compute, color=color, linestyle=\"solid\")\n",
    "            if show_transfer and not transfer_df.empty:\n",
    "                hop_count = data[\"details\"][region][\"route.hop_count\"]\n",
    "                carbon_route_raw_strings = data[\"details\"][region]['route']\n",
    "                router_hop_isos = '|'.join(filter(lambda x: x is not None, map(lambda x: parse_carbon_route(x, \"region\"), carbon_route_raw_strings)))\n",
    "                total_transfer_time = pd.to_timedelta(timings['total_transfer_time']).floor('s').to_pytimedelta()\n",
    "                if show_events:\n",
    "                    label_transfer = f\"{region} - Transfer ({carbon_emissions_transfer:.3f} gCO2 over {total_transfer_time})\"\n",
    "                else:\n",
    "                    label_transfer = f\"{region} - Transfer ({hop_count} hops: {router_hop_isos})\"\n",
    "                ax.step(transfer_df[\"Timestamp\"], transfer_df[\"Value\"], label=label_transfer, color=color, linestyle=\"dashed\")\n",
    "                if show_transfer_breakdown:\n",
    "                    ax.step(transfer_network_df[\"Timestamp\"], transfer_network_df[\"Value\"], label=f\"{region} - Transfer (network)\", color=color, linestyle=\"dotted\")\n",
    "                    ax.step(transfer_endpoint_df[\"Timestamp\"], transfer_endpoint_df[\"Value\"], label=f\"{region} - Transfer (endpoint)\", color=color, linestyle=\"dashdot\")\n",
    "\n",
    "            # Add events based on the timings\n",
    "            max_y_value = max(compute_df[\"Value\"].max(), transfer_df[\"Value\"].max())\n",
    "            for event in d_events if show_events else []:\n",
    "                df = compute_df if event == 'compute' else transfer_df\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                # Vertical lines and texts\n",
    "                for name in d_events[event]['interval_keys']:\n",
    "                    add_timing(ax, name, pd.to_datetime(timings[name]), max_y_value, color=d_region_colors[region])\n",
    "                # Fill area for events under the curve\n",
    "                (start_event, end_event) = d_events[event]['interval_keys']\n",
    "                mask = (df['Timestamp'] >= pd.to_datetime(timings[start_event])) & (df['Timestamp'] <= pd.to_datetime(timings[end_event]))\n",
    "                alpha = 0.5 if 'compute' in event else 0.25\n",
    "                ax.fill_between(x=df['Timestamp'], y1=df['Value'], where=mask, color=color, alpha=alpha)\n",
    "\n",
    "        ax.set_title(metadata[series_name]['title'])\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(metadata[series_name]['ylabel'])\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_carbon_route(route: str, output=\"coordinates\"):\n",
    "    # route = \"Router at (39.0127, -77.5342) (emap:US-MIDA-PJM)\"\n",
    "    regex = r\"Router at \\((?P<lat>-?\\d+\\.\\d+), (?P<lon>-?\\d+\\.\\d+)\\) \\(emap:(?P<region>.+)\\)\"\n",
    "    match = re.match(regex, route)\n",
    "    if match:\n",
    "        if output == \"coordinates\":\n",
    "            return (float(match.group(\"lat\")), float(match.group(\"lon\")))\n",
    "        elif output == \"region\":\n",
    "            return match.group(\"region\")\n",
    "        elif output is None:\n",
    "            return match.groupdict()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid output type: {output}\")\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_s = timedelta(seconds=3600).total_seconds()\n",
    "\n",
    "# See parameter definition in class `Workload` at https://github.com/c3lab-net/energy-data/blob/master/api/models/workload.py\n",
    "payload = {\n",
    "    \"runtime\": runtime_s,\n",
    "    \"schedule\": {\n",
    "        \"type\": \"onetime\",\n",
    "        \"start_time\": \"2022-01-02T00:00:00-00:00\",\n",
    "        \"max_delay\": 24*3600 - runtime_s\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"input_size_gb\": 150,\n",
    "        \"output_size_gb\": 150,\n",
    "    },\n",
    "    # Provide EITHER candidate_providers OR candidate_locations\n",
    "    \"candidate_providers\": [\n",
    "        \"AWS\"\n",
    "    ],\n",
    "    # \"candidate_locations\": [\n",
    "    #     {\n",
    "    #         \"id\": \"AWS:us-east-1\"\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"id\": \"AWS:us-west-1\"\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"id\": \"AWS:eu-central-1\"\n",
    "    #     },\n",
    "    # ],\n",
    "    \"use_prediction\": False,\n",
    "    # emap has the most coverage worldwide\n",
    "    \"carbon_data_source\": \"emap\",\n",
    "    \"watts_per_core\": 5,\n",
    "    \"core_count\": 20,\n",
    "    \"original_location\": \"AWS:us-east-1\",\n",
    "    # Note: turn off optimize_carbon if you don't need the timing information, e.g. just raw timeseries carbon data\n",
    "    \"optimize_carbon\": True,\n",
    "    # \"use_new_optimization\": True,   # defaults value, can ignore\n",
    "    # Most of the case we consider both compute and network\n",
    "    \"carbon_accounting_mode\": \"compute-and-network\",\n",
    "    # Accepted values are defined in enum `InterRegionRouteSource` at https://github.com/c3lab-net/energy-data/blob/master/api/models/cloud_location.py\n",
    "    \"inter_region_route_source\": \"itdk\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CARBON_API_URL='http://yak-03.sysnet.ucsd.edu/carbon-aware-scheduler/'\n",
    "\n",
    "# Make the API call\n",
    "response = requests.get(CARBON_API_URL, json=payload)\n",
    "\n",
    "# Check if the API call was successful (status code 200)\n",
    "assert response.ok, f\"Error: API call failed with status code {response.status_code}: {response.text}\"\n",
    "data = response.json(parse_float=lambda s: float('%.6g' % float(s)))\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all information (works best without too many regions)\n",
    "# plot_carbon_api_response(data)\n",
    "\n",
    "# Show only transfer without breakdown\n",
    "plot_carbon_api_response(data, show_events=False, show_transfer_breakdown=False, show_compute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile individual workloads\n",
    "\n",
    "payload = {\n",
    "    \"runtime\": 3600,\n",
    "    \"schedule\": {\n",
    "        \"type\": \"onetime\",\n",
    "        \"start_time\": \"2022-01-02T00:00:00-07:00\",\n",
    "        \"max_delay\": 3600 * 24 - 3600\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"input_size_gb\": 20,\n",
    "        \"output_size_gb\": 12\n",
    "    },\n",
    "    \"candidate_locations\": [\n",
    "        {\n",
    "            \"id\": \"AWS:us-east-1\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"AWS:us-west-1\"\n",
    "        },\n",
    "    ],\n",
    "    \"use_prediction\": False,\n",
    "    \"carbon_data_source\": \"emap\",\n",
    "    \"watts_per_core\": 125/20,\n",
    "    \"core_count\": 1,\n",
    "    \"original_location\": \"AWS:us-east-1\",\n",
    "    \"carbon_accounting_mode\": \"compute-and-network\",\n",
    "    \"optimize_carbon\": True,\n",
    "    \"use_new_optimization\": True,\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Workload:\n",
    "    name: str\n",
    "    core_count: int\n",
    "    runtime_s: float\n",
    "    input_gb: float\n",
    "    output_gb: float\n",
    "\n",
    "\"\"\"csv\n",
    "name,core_count,runtime_s,input_gb,output_gb\n",
    "compression (gzip),13,2.1,1.2,0.19\n",
    "compression (bzip),31,3.2,1.2,0.142\n",
    "code compilation (linux v5.16),40,391,1.2,0.195\n",
    "video resize (4k->1080p),4,261,4.6,0.251\n",
    "video effect (4k, h.264),8,307,1.2,0.122\n",
    "video effect (4k, h.265),8,307,0.15,0.117\n",
    "ML inference (resnet50, 1k images),1,246,0.154,0\n",
    "\"\"\"\n",
    "\n",
    "profiled_workloads = [\n",
    "    Workload(\"compression (gzip)\", 13, 2.1, 1.2, 0.19),\n",
    "    Workload(\"compression (bzip)\", 31, 3.2, 1.2, 0.142),\n",
    "    Workload(\"code compilation (linux v5.16)\", 40, 391, 1.2, 0.195),\n",
    "    Workload(\"video resize (4k->1080p)\", 4, 261, 4.6, 0.251),\n",
    "    Workload(\"video effect (4k, h.264)\", 8, 307, 1.2, 0.122),\n",
    "    Workload(\"video effect (4k, h.265)\", 8, 307, 0.15, 0.117),\n",
    "    Workload(\"ML inference (resnet50, 1k images)\", 1, 246, 0.154, 0),\n",
    "]\n",
    "\n",
    "CARBON_API_URL = 'http://yak-03.sysnet.ucsd.edu/carbon-aware-scheduler/'\n",
    "\n",
    "for workload in profiled_workloads:\n",
    "    workload.runtime_s *= 10\n",
    "    workload.input_gb *= 10\n",
    "    workload.output_gb *= 10\n",
    "    payload['runtime'] = workload.runtime_s\n",
    "    payload['schedule']['max_delay'] = timedelta(days=1).total_seconds() - workload.runtime_s\n",
    "    payload['dataset']['input_size_gb'] = workload.input_gb\n",
    "    payload['dataset']['output_size_gb'] = workload.output_gb\n",
    "    payload['core_count'] = workload.core_count\n",
    "    payload['use_new_optimization'] = False\n",
    "    print(f'{workload.name} - {workload.core_count}c * {workload.runtime_s}s', f'{workload.input_gb}G/{workload.output_gb}G')\n",
    "    print('Core-hours/GB:', workload.core_count * workload.runtime_s / timedelta(hours=1).total_seconds() / (workload.input_gb + workload.output_gb))\n",
    "\n",
    "    response = requests.get(CARBON_API_URL, json=payload)\n",
    "    assert response.ok, f\"Error: API call failed with status code {response.status_code}: {response.text}\"\n",
    "    data = response.json(parse_float=lambda s: float('%.6g' % float(s)))\n",
    "    # print(json.dumps(data, indent=4))\n",
    "    plot_carbon_api_response(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ad-hoc: verify that new optimization is no worse than the old one, by sweeping over all parameters\n",
    "\n",
    "payload = {\n",
    "    \"runtime\": 5400,\n",
    "    \"schedule\": {\n",
    "        \"type\": \"onetime\",\n",
    "        \"start_time\": \"2023-05-24T22:00:00-05:00\",\n",
    "        \"max_delay\": 36000\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"input_size_gb\": 20,\n",
    "        \"output_size_gb\": 12\n",
    "    },\n",
    "    \"candidate_locations\": [\n",
    "        {\n",
    "            \"id\": \"AWS:us-east-1\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"AWS:us-west-1\"\n",
    "        },\n",
    "        # {\n",
    "        #     \"id\": \"AWS:us-west-1\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"id\": \"AWS:us-west-1\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"id\": \"AWS:us-west-1\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"id\": \"AWS:us-west-1\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"id\": \"AWS:us-west-1\"\n",
    "        # },\n",
    "    ],\n",
    "    \"use_prediction\": False,\n",
    "    \"carbon_data_source\": \"emap\",\n",
    "    \"watts_per_core\": 5,\n",
    "    \"core_count\": 40,\n",
    "    \"original_location\": \"AWS:us-east-1\",\n",
    "    \"carbon_accounting_mode\": \"compute-and-network\",\n",
    "    \"optimize_carbon\": True,\n",
    "}\n",
    "\n",
    "\n",
    "def compare_results(payload1: dict, payload2: dict):\n",
    "    response1 = requests.get(CARBON_API_URL, json=payload1)\n",
    "    response2 = requests.get(CARBON_API_URL, json=payload2)\n",
    "    if response1.ok != response2.ok:\n",
    "        print(\"Inconsistent response status\")\n",
    "        print(payload)\n",
    "        print(response1)\n",
    "        print(response2)\n",
    "        return\n",
    "    elif response1.ok == False:\n",
    "        print(\"Both responses are not ok\")\n",
    "        print(payload)\n",
    "        print(response1.status_code)\n",
    "        print(response1.json())\n",
    "        return\n",
    "    parse_float=lambda s: float('%.9g' % float(s))\n",
    "    json1 = response1.json(parse_float=parse_float)\n",
    "    json2 = response2.json(parse_float=parse_float)\n",
    "    del json1['request']\n",
    "    del json2['request']\n",
    "    if json1 != json2:\n",
    "        should_report = False\n",
    "        for region in json2['raw-scores']:\n",
    "            if json2['raw-scores'][region]['carbon-emission'] > json1['raw-scores'][region]['carbon-emission']:\n",
    "                print(f\"Worse result for {region}\")\n",
    "                should_report = True\n",
    "        if should_report:\n",
    "            print(\"Inconsistent response content\")\n",
    "            print(payload)\n",
    "            print(json1)\n",
    "            print(json2)\n",
    "            return\n",
    "    else:\n",
    "        print(\"Same result\")\n",
    "    print(json1)\n",
    "    print(json2)\n",
    "    print('old:', end='')\n",
    "    for region in json1['raw-scores']:\n",
    "        print('\\t', region, json1['raw-scores'][region]['carbon-emission'], end='')\n",
    "    print()\n",
    "    print('new:', end='')\n",
    "    for region in json2['raw-scores']:\n",
    "        print('\\t', region, json2['raw-scores'][region]['carbon-emission'], end='')\n",
    "    print()\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "CARBON_API_URL='http://yak-03.sysnet.ucsd.edu/carbon-aware-scheduler/'\n",
    "\n",
    "# # Fuzzing errorneous input\n",
    "# payloads = [\n",
    "#     {'runtime': 14802, 'schedule': {'type': 'onetime', 'start_time': '2022-08-18T12:18:57+00:00', 'max_delay': 36000}, 'dataset': {'input_size_gb': 20, 'output_size_gb': 69}, 'candidate_locations': [{'id': 'AWS:us-east-1'}, {'id': 'AWS:us-west-1'}], 'use_prediction': False, 'carbon_data_source': 'emap', 'watts_per_core': 5, 'core_count': 5, 'original_location': 'AWS:us-east-1', 'carbon_accounting_mode': 'compute-and-network', 'optimize_carbon': True, 'use_new_optimization': False},\n",
    "# ]\n",
    "\n",
    "# for payload in payloads:\n",
    "#     payload_new_optimization = copy.deepcopy(payload)\n",
    "#     payload_new_optimization['use_new_optimization'] = True\n",
    "#     compare_results(payload, payload_new_optimization)\n",
    "\n",
    "# Make the API call\n",
    "for i in range(365):\n",
    "    for runtime_seconds in [random.randint(1, 3600 * 24) for _ in range(1)]:\n",
    "        start_time = datetime(2022, 1, 1, 0, 0, 0, tzinfo=timezone.utc) + timedelta(days=i)\n",
    "        start_time += timedelta(seconds=random.randint(0, 3600 * 24))\n",
    "        data_input_size_gb = random.randint(1, 100)\n",
    "        data_output_size_gb = random.randint(1, 100)\n",
    "        core_count = random.randint(1, 100)\n",
    "\n",
    "        payload['schedule']['start_time'] = start_time.isoformat()\n",
    "        payload['runtime'] = runtime_seconds\n",
    "        payload['dataset']['input_size_gb'] = data_input_size_gb\n",
    "        payload['dataset']['output_size_gb'] = data_output_size_gb\n",
    "        payload['core_count'] = core_count\n",
    "        payload['use_new_optimization'] = False\n",
    "        print(start_time, f'{core_count}c * {runtime_seconds}s', f'{data_input_size_gb}G/{data_output_size_gb}G')\n",
    "\n",
    "        payload_new_optimization = copy.deepcopy(payload)\n",
    "        payload_new_optimization['use_new_optimization'] = True\n",
    "        compare_results(payload, payload_new_optimization)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
