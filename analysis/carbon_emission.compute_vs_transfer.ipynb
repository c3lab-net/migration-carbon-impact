{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "import copy\n",
    "import random\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import date, datetime, timezone, timedelta\n",
    "import dateutil\n",
    "from matplotlib_helper import *\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "\n",
    "metadata = {\n",
    "    'emission_rates': {\n",
    "        'ylabel': 'gCO2/s',\n",
    "        'title': 'Instantaneous emission rates'\n",
    "    },\n",
    "    'emission_integral': {\n",
    "        'ylabel': 'gCO2',\n",
    "        'title': 'Emission integral over its duration'\n",
    "    },\n",
    "}\n",
    "\n",
    "d_timing_labels = {\n",
    "    \"input_transfer_start\": \"Input transfer\",\n",
    "    # \"input_transfer_start\": \"Start of input transfer\",\n",
    "    # \"input_transfer_end\": \"End of input transfer\",\n",
    "    \"compute_start\": \"Compute\",\n",
    "    # \"compute_start\": \"Start of compute\",\n",
    "    # \"compute_end\": \"End of compute\",\n",
    "    \"output_transfer_start\": \"Output transfer\",\n",
    "    # \"output_transfer_start\": \"Start of output transfer\",\n",
    "    # \"output_transfer_end\": \"End of output transfer\",\n",
    "}\n",
    "\n",
    "d_events = {\n",
    "    'input_transfer': {\n",
    "        'interval_keys': (\"input_transfer_start\", \"input_transfer_end\"),\n",
    "        'label': 'Input transfer',\n",
    "    },\n",
    "    'compute': {\n",
    "        'interval_keys': (\"compute_start\", \"compute_end\"),\n",
    "        'label': 'Compute',\n",
    "    },\n",
    "    'output_transfer': {\n",
    "        'interval_keys': (\"output_transfer_start\", \"output_transfer_end\"),\n",
    "        'label': 'Output transfer',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_value(data_details: dict, series_name: str):\n",
    "    max_value = 0\n",
    "    for region in data_details:\n",
    "        compute_data = data_details[region][series_name][\"compute\"]\n",
    "        transfer_data = data_details[region][series_name][\"transfer\"]\n",
    "        max_value = max(max_value, max(compute_data.values(), default=0), max(transfer_data.values(), default=0))\n",
    "    return max_value\n",
    "\n",
    "def resample_timeseries(df: pd.DataFrame, interval: str):\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "    df.set_index(\"Timestamp\", inplace=True)\n",
    "    df_resampled = df.resample(interval).ffill().reset_index()\n",
    "    return df_resampled\n",
    "\n",
    "def create_dataframe_for_plotting(timeseries: dict[str, float], min_start: datetime, max_end: datetime) -> pd.DataFrame:\n",
    "    \"\"\"Convert a time series data to a dataframe, while removing out of bound timestamps.\n",
    "    \n",
    "        Args:\n",
    "            timeseries: A dictionary of timestamp strings and values.\n",
    "            min_start: The minimum cutoff time for the timeseries.\n",
    "            max_end: The maximum cutoff time for the timeseries.\n",
    "    \"\"\"\n",
    "    timeseries_in_datatime = {datetime.fromisoformat(key): value for key, value in timeseries.items()}\n",
    "    df = pd.DataFrame(list(timeseries_in_datatime.items()), columns=[\"Timestamp\", \"Value\"])\n",
    "    if df.empty:\n",
    "        return df\n",
    "    resampled = resample_timeseries(df, \"30s\")\n",
    "    mask = (resampled[\"Timestamp\"] >= pd.to_datetime(min_start)) & (resampled[\"Timestamp\"] <= pd.to_datetime(max_end))\n",
    "    return resampled[mask]\n",
    "\n",
    "def add_timing(ax, name: str, time: pd.Timestamp, max_value: float, color: str):\n",
    "    if 'start' in name:\n",
    "        ax.vlines(time, ymin=0, ymax=max_value, color='gray', alpha=0.5, linestyles=\"solid\" if 'compute' in name else \"dashed\")\n",
    "        if 'input' in name:\n",
    "            ha = 'right'\n",
    "            rotation = -30\n",
    "        elif 'output' in name:\n",
    "            ha = 'left'\n",
    "            rotation = 30\n",
    "        else:\n",
    "            ha = 'center'\n",
    "            rotation = 0\n",
    "        ax.text(time, max_value, d_timing_labels[name], color=color, alpha=0.95, ha=ha, va=\"bottom\", rotation=rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "def plot_carbon_api_response(data: dict, show_events=True, show_transfer_breakdown = True, show_compute=True, show_transfer=True):\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "    d_region_colors = {}\n",
    "    for region in data['details']:\n",
    "        d_region_colors[region] = get_next_color()\n",
    "\n",
    "    # Extract emission integral data\n",
    "    for (ax, series_name) in zip([axes], [\"emission_rates\", \"emission_integral\"]):\n",
    "        if series_name == 'emission_integral':\n",
    "            continue\n",
    "        # # Get max value for y-axis\n",
    "        # max_y_value = get_max_value(data['details'], series_name)\n",
    "\n",
    "        for region in data['details']:\n",
    "            # if region == 'Azure:eastus':\n",
    "            #     continue\n",
    "            print(f\"Plotting {region} - {series_name}\")\n",
    "            compute_data = data[\"details\"][region][series_name][\"compute\"]\n",
    "            transfer_data = data[\"details\"][region][series_name][\"transfer\"]\n",
    "            transfer_network_data = data[\"details\"][region][series_name][\"transfer.network\"]\n",
    "            transfer_endpoint_data = data[\"details\"][region][series_name][\"transfer.endpoint\"]\n",
    "            timings = data[\"details\"][region]['timings'][0] # Assume single occurence per job\n",
    "            min_start = datetime.fromisoformat(timings['min_start'])\n",
    "            max_end = datetime.fromisoformat(timings['max_end'])\n",
    "            carbon_emissions_compute = data['raw-scores'][region]['carbon-emission-from-compute']\n",
    "            carbon_emissions_transfer = data['raw-scores'][region]['carbon-emission-from-migration']\n",
    "\n",
    "            # Convert timestamp strings to datetime objects\n",
    "            compute_df = create_dataframe_for_plotting(compute_data, min_start, max_end)\n",
    "            transfer_df = create_dataframe_for_plotting(transfer_data, min_start, max_end)\n",
    "            transfer_network_df = create_dataframe_for_plotting(transfer_network_data, min_start, max_end)\n",
    "            transfer_endpoint_df = create_dataframe_for_plotting(transfer_endpoint_data, min_start, max_end)\n",
    "\n",
    "            # Plot timeseries data as step functions\n",
    "            color = d_region_colors[region]\n",
    "            if show_compute:\n",
    "                compute_time = pd.to_timedelta(timings['compute_duration']).floor('s').to_pytimedelta()\n",
    "                label_compute = f\"{region} - Compute ({carbon_emissions_compute:.3f} gCO2 over {compute_time})\"\n",
    "                ax.step(compute_df[\"Timestamp\"], compute_df[\"Value\"], label=label_compute, color=color, linestyle=\"solid\")\n",
    "            if show_transfer and not transfer_df.empty:\n",
    "                hop_count = data[\"details\"][region][\"route.hop_count\"]\n",
    "                carbon_route_raw_strings = data[\"details\"][region]['route']\n",
    "                router_hop_isos = '|'.join(filter(lambda x: x is not None, map(lambda x: parse_carbon_route(x, \"region\"), carbon_route_raw_strings)))\n",
    "                total_transfer_time = pd.to_timedelta(timings['total_transfer_time']).floor('s').to_pytimedelta()\n",
    "                if show_events:\n",
    "                    label_transfer = f\"{region} - Transfer ({carbon_emissions_transfer:.3f} gCO2 over {total_transfer_time})\"\n",
    "                else:\n",
    "                    label_transfer = f\"{region} - Transfer ({hop_count} hops: {router_hop_isos})\"\n",
    "                ax.step(transfer_df[\"Timestamp\"], transfer_df[\"Value\"], label=label_transfer, color=color, linestyle=\"dashed\")\n",
    "                if show_transfer_breakdown:\n",
    "                    ax.step(transfer_network_df[\"Timestamp\"], transfer_network_df[\"Value\"], label=f\"{region} - Transfer (network)\", color=color, linestyle=\"dotted\")\n",
    "                    ax.step(transfer_endpoint_df[\"Timestamp\"], transfer_endpoint_df[\"Value\"], label=f\"{region} - Transfer (endpoint)\", color=color, linestyle=\"dashdot\")\n",
    "\n",
    "            # Add events based on the timings\n",
    "            max_y_value = max(compute_df[\"Value\"].max(), transfer_df[\"Value\"].max())\n",
    "            for event in d_events if show_events else []:\n",
    "                df = compute_df if event == 'compute' else transfer_df\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                # Vertical lines and texts\n",
    "                for name in d_events[event]['interval_keys']:\n",
    "                    add_timing(ax, name, pd.to_datetime(timings[name]), max_y_value, color=d_region_colors[region])\n",
    "                # Fill area for events under the curve\n",
    "                (start_event, end_event) = d_events[event]['interval_keys']\n",
    "                mask = (df['Timestamp'] >= pd.to_datetime(timings[start_event])) & (df['Timestamp'] <= pd.to_datetime(timings[end_event]))\n",
    "                alpha = 0.5 if 'compute' in event else 0.25\n",
    "                ax.fill_between(x=df['Timestamp'], y1=df['Value'], where=mask, color=color, alpha=alpha)\n",
    "\n",
    "        ax.set_title(metadata[series_name]['title'])\n",
    "        ax.set_xlabel(\"Time\")\n",
    "        ax.set_ylabel(metadata[series_name]['ylabel'])\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_carbon_route(route: str, output=\"coordinates\"):\n",
    "    # route = \"Router at (39.0127, -77.5342) (emap:US-MIDA-PJM)\"\n",
    "    regex = r\"Router at \\((?P<lat>-?\\d+\\.\\d+), (?P<lon>-?\\d+\\.\\d+)\\) \\(emap:(?P<region>.+)\\)\"\n",
    "    match = re.match(regex, route)\n",
    "    if match:\n",
    "        if output == \"coordinates\":\n",
    "            return (float(match.group(\"lat\")), float(match.group(\"lon\")))\n",
    "        elif output == \"region\":\n",
    "            return match.group(\"region\")\n",
    "        elif output is None:\n",
    "            return match.groupdict()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid output type: {output}\")\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all regions that we cover\n",
    "\n",
    "CLOUD_PROVIDERS = ['AWS', 'gcloud']\n",
    "CARBON_API_HOST = 'http://yak-03.sysnet.ucsd.edu'\n",
    "\n",
    "cloud_regions_by_provider: dict[str, list[str]] = {}\n",
    "\n",
    "def get_cloud_regions(cloud_provider: str) -> list[str]:\n",
    "    \"\"\"Get a list of regions for a given cloud provider.\n",
    "    \n",
    "        Args:\n",
    "            cloud_provider: The cloud provider name.\n",
    "    \"\"\"\n",
    "    response = requests.get(f\"{CARBON_API_HOST}/metadata/cloud-location/locations/{cloud_provider}/\")\n",
    "    assert response.ok, f\"Error: API call failed with status code {response.status_code}: {response.text}\"\n",
    "    regions = response.json()\n",
    "    return list(regions)\n",
    "\n",
    "for cloud_provider in CLOUD_PROVIDERS:\n",
    "    cloud_regions = get_cloud_regions(cloud_provider)\n",
    "    cloud_regions_by_provider[cloud_provider] = cloud_regions\n",
    "    print(f\"{cloud_provider}: {len(cloud_regions)} regions\")\n",
    "\n",
    "# Because they don't have yearly data on the endpoints, we'll ignore these regions\n",
    "EXCLUDED_REGION_IDS = [\n",
    "    \"AWS:ap-northeast-1\",\n",
    "    \"gcloud:asia-northeast1\",\n",
    "    \"gcloud:europe-west6\",\n",
    "]\n",
    "\n",
    "ALL_REGION_IDS = []\n",
    "for cloud in CLOUD_PROVIDERS:\n",
    "    for region in cloud_regions_by_provider[cloud]:\n",
    "        region_id = f\"{cloud}:{region}\"\n",
    "        if region_id in EXCLUDED_REGION_IDS:\n",
    "            continue\n",
    "        ALL_REGION_IDS.append(region_id)\n",
    "\n",
    "print('Total regions used:', len(ALL_REGION_IDS))\n",
    "\n",
    "# pprint.pprint(cloud_regions_by_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "for region_id in ALL_REGION_IDS:\n",
    "    print(region_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A couple of test payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_s = timedelta(seconds=3600).total_seconds()\n",
    "\n",
    "# See parameter definition in class `Workload` at https://github.com/c3lab-net/energy-data/blob/master/api/models/workload.py\n",
    "payload = {\n",
    "    \"runtime\": runtime_s,\n",
    "    \"schedule\": {\n",
    "        \"type\": \"onetime\",\n",
    "        \"start_time\": \"2022-01-02T00:00:00-00:00\",\n",
    "        \"max_delay\": 24*3600 - runtime_s\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"input_size_gb\": 512,\n",
    "        \"output_size_gb\": 512,\n",
    "    },\n",
    "    # Provide EITHER candidate_providers OR candidate_locations\n",
    "    \"candidate_providers\": [\n",
    "        \"AWS\",\n",
    "        \"gcloud\",\n",
    "    ],\n",
    "    # \"candidate_locations\": [\n",
    "    #     {\n",
    "    #         \"id\": \"AWS:us-east-1\"\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"id\": \"AWS:us-west-1\"\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"id\": \"AWS:eu-central-1\"\n",
    "    #     },\n",
    "    # ],\n",
    "    \"use_prediction\": False,\n",
    "    # emap has the most coverage worldwide\n",
    "    \"carbon_data_source\": \"emap\",\n",
    "    \"watts_per_core\": 0,\n",
    "    \"core_count\": 20,\n",
    "    \"original_location\": \"AWS:us-east-1\",\n",
    "    # Note: turn off optimize_carbon if you don't need the timing information, e.g. just raw timeseries carbon data\n",
    "    \"optimize_carbon\": False,\n",
    "    # \"use_new_optimization\": True,   # defaults value, can ignore\n",
    "    # Most of the case we consider both compute and network\n",
    "    \"carbon_accounting_mode\": \"compute-and-network\",\n",
    "    # Accepted values are defined in enum `InterRegionRouteSource` at https://github.com/c3lab-net/energy-data/blob/master/api/models/cloud_location.py\n",
    "    \"inter_region_route_source\": \"igdb.no-pops\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'runtime': 141.0, 'schedule': {'type': 'onetime', 'start_time': '2022-04-10T08:18:40+00:00', 'max_delay': 82800.0}, 'dataset': {'input_size_gb': 0.03356373688888889, 'output_size_gb': 0.009619868777777778}, 'candidate_providers': ['AWS'], 'use_prediction': False, 'carbon_data_source': 'emap', 'watts_per_core': 5, 'core_count': 1.52, 'original_location': 'AWS:us-east-1', 'optimize_carbon': True, 'carbon_accounting_mode': 'compute-and-network', 'inter_region_route_source': 'itdk+igdb.no-pops'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = timedelta(seconds=3600).total_seconds()\n",
    "interval = timedelta(days=1).total_seconds()\n",
    "max_delay = interval - runtime\n",
    "\n",
    "payload = {'runtime': runtime, 'schedule': {'type': 'onetime', 'start_time': '2022-01-01T00:00:00+00:00', 'max_delay': max_delay}, 'dataset': {'input_size_gb': 1, 'output_size_gb': 1}, 'candidate_providers': ['AWS', 'gcloud'], 'use_prediction': False, 'carbon_data_source': 'emap', 'watts_per_core': 1, 'core_count': 1, 'original_location': 'AWS:us-east-1', 'optimize_carbon': False, 'carbon_accounting_mode': 'compute-and-network', 'inter_region_route_source': 'itdk+igdb.no-pops'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_payload(payload: dict, src: str, dsts: list[str] = []):\n",
    "    payload['original_location'] = src\n",
    "    if dsts:\n",
    "        if 'candidate_providers' in payload:\n",
    "            del payload['candidate_providers']\n",
    "        payload['candidate_locations'] = [{'id': dst} for dst in dsts]\n",
    "    else:\n",
    "        if 'candidate_locations' in payload:\n",
    "            del payload['candidate_locations']\n",
    "        payload['candidate_providers'] = CLOUD_PROVIDERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify we have data for all the ISOs in 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_route_iso = re.compile(r'([\\w ]+) at \\(([-\\d.]+), ([-\\d.]+)\\) \\(emap:([\\w-]+)\\)')\n",
    "\n",
    "def get_iso_from_hop_str(hop: str) -> str:\n",
    "    # Example hop: \"Router at (39.0469, -77.4903) (emap:US-MIDA-PJM)\"\n",
    "    m = regex_route_iso.match(hop)\n",
    "    if m:\n",
    "        return m.group(4)\n",
    "    else:\n",
    "        raise ValueError(f'Cannot parse hop: {hop}')\n",
    "\n",
    "def reverse_region_to_isos_mapping(d: dict[str, list[str]], remove_prefix: str = None) -> dict[str, list[str]]:\n",
    "    reversed: dict[str, list[str]] = defaultdict(set)\n",
    "    for region, isos in d.items():\n",
    "        for iso in isos:\n",
    "            if remove_prefix and iso.startswith(remove_prefix):\n",
    "                iso = iso.removeprefix(remove_prefix)\n",
    "            reversed[iso].add(region)\n",
    "    return reversed\n",
    "\n",
    "def parse_warning_no_carbon_data_to_isos(message: str) -> list[str]:\n",
    "    # Example message: \"No carbon data found for iso emap:DZ in time range ...\"\n",
    "    # or \"Carbon data not available for iso emap:JP-TK for the entire time range [2023-11-09 00:00:00+00:00, 2023-11-10 00:00:00+00:00]\"\n",
    "    regex = r\"(?:No carbon data found for|Carbon data not available for) iso ([\\w:-]+) (?:in time range|for the entire time range)\"\n",
    "    match = re.match(regex, message)\n",
    "    if match:\n",
    "        return [match.group(1)]\n",
    "    # or \"ISOs with missing carbon data (1): emap:AO, emap:XX.\"\n",
    "    regex_multiple_isos = r\"ISOs with missing carbon data \\(\\d+\\): ([\\w:, -].+)\"\n",
    "    for line in message.splitlines():\n",
    "        match = re.match(regex_multiple_isos, line, re.MULTILINE)\n",
    "        if match:\n",
    "            return match.group(1).split(', ')\n",
    "    raise ValueError(f'Cannot parse warning message: {message}')\n",
    "\n",
    "def get_iso_to_affected_region_pairs(data) -> dict[str, list[tuple[str, str]]]:\n",
    "    \"\"\"Get the unique ISO and the affected region pairs from the both regions and transit hops.\n",
    "    \n",
    "        Args:\n",
    "            data: The carbon API response.\n",
    "    \"\"\"\n",
    "    # Region route ISOs\n",
    "    region_to_isos = defaultdict(list)\n",
    "    for region, d in data['details'].items():\n",
    "        region_to_isos[region] = [get_iso_from_hop_str(hop) for hop in d['route']]\n",
    "\n",
    "    # Regions with no carbon data\n",
    "    if 'warnings' in data:\n",
    "        for region, message in data['warnings'].items():\n",
    "            region_to_isos[region] += parse_warning_no_carbon_data_to_isos(message)\n",
    "\n",
    "    # Region's local ISO\n",
    "    for region, iso in data['isos'].items():\n",
    "        region_to_isos[region].append(iso)\n",
    "\n",
    "    original_location = data['request']['original_location']\n",
    "    return { iso: [(original_location, region) for region in regions if original_location != region] \\\n",
    "            for iso, regions in reverse_region_to_isos_mapping(region_to_isos, 'emap:').items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From database, the list of zoneids with full 2023 data\n",
    "emap_zone_ids_with_full_2023_data = [\n",
    "    'AT', 'AU-NSW', 'AU-NT', 'AU-QLD', 'AU-SA', 'AU-TAS', 'AU-VIC', 'AU-WA', 'BA', 'BE', 'BG', 'BR', 'BR-CS', 'BR-N', 'BR-NE', 'BR-S', 'CA-ON', 'CA-QC', 'CL-SEN', 'CR', 'CY', 'CZ', 'DE', 'DK', 'DK-BHM', 'DK-DK1', 'DK-DK2', 'EE', 'ES', 'FI', 'FR', 'GB', 'GB-NIR', 'GR', 'HK', 'HR', 'HU', 'ID', 'IE', 'IL', 'IN-EA', 'IN-NE', 'IN-NO', 'IN-SO', 'IN-WE', 'IS', 'IT', 'IT-CNO', 'IT-CSO', 'IT-NO', 'IT-SAR', 'IT-SIC', 'IT-SO', 'JP', 'JP-CB', 'JP-CG', 'JP-HKD', 'JP-HR', 'JP-KN', 'JP-KY', 'JP-ON', 'JP-TH', 'KR', 'LT', 'LU', 'LV', 'NI', 'NL', 'NO', 'NO-NO1', 'NO-NO2', 'NO-NO3', 'NO-NO4', 'NO-NO5', 'NZ', 'PA', 'PE', 'PH', 'PH-LU', 'PH-MI', 'PH-VI', 'PL', 'PT', 'RO', 'RS', 'SE', 'SE-SE1', 'SE-SE2', 'SE-SE3', 'SE-SE4', 'SG', 'SI', 'SK', 'TR', 'TW', 'US', 'US-CAL-BANC', 'US-CAL-CISO', 'US-CAL-IID', 'US-CAL-LDWP', 'US-CAL-TIDC', 'US-CAR-CPLE', 'US-CAR-CPLW', 'US-CAR-DUK', 'US-CAR-SC', 'US-CAR-SCEG', 'US-CAR-YAD', 'US-CENT-SPA', 'US-CENT-SWPP', 'US-FLA-FMPP', 'US-FLA-FPC', 'US-FLA-FPL', 'US-FLA-GVL', 'US-FLA-JEA', 'US-FLA-SEC', 'US-FLA-TAL', 'US-FLA-TEC', 'US-MIDA-PJM', 'US-MIDW-AECI', 'US-MIDW-LGEE', 'US-MIDW-MISO', 'US-NE-ISNE', 'US-NW-AVA', 'US-NW-BPAT', 'US-NW-CHPD', 'US-NW-DOPD', 'US-NW-GCPD', 'US-NW-GRID', 'US-NW-IPCO', 'US-NW-NEVP', 'US-NW-NWMT', 'US-NW-PACE', 'US-NW-PACW', 'US-NW-PGE', 'US-NW-PSCO', 'US-NW-PSEI', 'US-NW-SCL', 'US-NW-TPWR', 'US-NW-WACM', 'US-NW-WAUW', 'US-NY-NYIS', 'US-SE-SOCO', 'US-SW-AZPS', 'US-SW-EPE', 'US-SW-PNM', 'US-SW-SRP', 'US-SW-TEPC', 'US-SW-WALC', 'US-TEN-TVA', 'US-TEX-ERCO', 'UY', 'ZA',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# _debug_\n",
    "\n",
    "payload = {'runtime': 3600.0, 'schedule': {'type': 'onetime', 'start_time': '2022-12-01T00:00:00+00:00', 'max_delay': 3600*24 - 3600}, 'dataset': {'input_size_gb': 1024, 'output_size_gb': 1024}, 'candidate_providers': ['AWS', 'gcloud'], 'use_prediction': False, 'carbon_data_source': 'emap', 'watts_per_core': 0, 'core_count': 20, 'original_location': 'AWS:us-east-1', 'optimize_carbon': False, 'carbon_accounting_mode': 'compute-and-network', 'inter_region_route_source': 'itdk'}\n",
    "response = requests.get(CARBON_API_URL, json=payload)\n",
    "\n",
    "# Check if the API call was successful (status code 200)\n",
    "assert response.ok, f\"Error: API call failed with status code {response.status_code}: {response.text}\"\n",
    "data = response.json(parse_float=lambda s: float('%.6g' % float(s)))\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAYLOAD_FULL_DAY_NETWORK_TRANSFER_ONLY = {\n",
    "    \"runtime\": 3600,\n",
    "    \"schedule\": {\n",
    "        \"type\": \"onetime\",\n",
    "        \"start_time\": \"2023-01-01T00:00:00+00:00\",\n",
    "        \"interval\": None,\n",
    "        \"max_delay\": 3600 * 24 - 3600\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"input_size_gb\": 1,\n",
    "        \"output_size_gb\": 1\n",
    "    },\n",
    "    \"original_location\": None,\n",
    "    \"candidate_providers\": [\n",
    "        \"AWS\",\n",
    "        \"gcloud\"\n",
    "    ],\n",
    "    \"candidate_locations\": [],\n",
    "    \"carbon_data_source\": \"emap\",\n",
    "    \"use_prediction\": False,\n",
    "    \"desired_renewable_ratio\": None,\n",
    "    \"optimize_carbon\": False,\n",
    "    \"watts_per_core\": 0,\n",
    "    \"core_count\": 1,\n",
    "    \"use_new_optimization\": True,\n",
    "    \"carbon_accounting_mode\": \"compute-and-network\",\n",
    "    \"inter_region_route_source\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the route estimation heuristics (when certain hops have missing carbon data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL_ROUTE_SOURCES = ['itdk', 'igdb.no-pops', 'igdb.with-pops', 'itdk+igdb.no-pops', 'itdk+igdb.with-pops']\n",
    "ALL_ROUTE_SOURCES = ['itdk+igdb.no-pops', 'itdk+igdb.with-pops']\n",
    "# All zone coverage starts immediately after 2023/11/21 01:00:00 UTC\n",
    "START_TIME = datetime(2023, 11, 22, 0, 0, 0, tzinfo=timezone.utc)\n",
    "DAYS_TO_COVER = 60\n",
    "\n",
    "CARBON_API_URL='http://yak-03.sysnet.ucsd.edu/carbon-aware-scheduler/'\n",
    "THREAD_COUNT = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from typing import Any, Optional\n",
    "\n",
    "\n",
    "def _call_carbon_api_no_throw(payload: dict) -> tuple[Optional[dict], Optional[Any]]:\n",
    "    \"\"\"Returns a pair of response JSON and any error message.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(CARBON_API_URL, json=payload, timeout=60)\n",
    "    except TimeoutError as e:\n",
    "        return { \"request\": payload }, f\"Error: API call timed out: {e}\"\n",
    "    if response.ok:\n",
    "        try:\n",
    "            return response.json(parse_float=lambda s: float('%.6g' % float(s))), None\n",
    "        except Exception as e:\n",
    "            return { \"request\": payload }, f\"Error: Failed to parse response JSON: {e}\"\n",
    "    else:\n",
    "        if response.status_code == 400:\n",
    "            return response.json(parse_float=lambda s: float('%.6g' % float(s))), None\n",
    "        else:\n",
    "            return { \"request\": payload }, f\"Error: API call failed with status code {response.status_code}: {response.text}\"\n",
    "    #end\n",
    "\n",
    "def set_estimation_heuristic(payload: dict, on: bool,\n",
    "                             minimum_known_carbon_power_ratio: Optional[float] = None,\n",
    "                             heuristic: Optional[str] = None,\n",
    "                             param: Optional[Any] = None):\n",
    "    if 'network_hop_carbon_estimation_heuristic' in payload:\n",
    "        del payload['network_hop_carbon_estimation_heuristic']\n",
    "\n",
    "    if not on:\n",
    "        payload['network_hop_carbon_estimation_heuristic'] = 'no-estimation'\n",
    "    else:\n",
    "        assert minimum_known_carbon_power_ratio is not None\n",
    "        payload['network_hop_carbon_estimation_minimum_known_carbon_power_ratio'] = \\\n",
    "            minimum_known_carbon_power_ratio\n",
    "\n",
    "        assert heuristic is not None\n",
    "        if heuristic == 'route-average':\n",
    "            payload['network_hop_carbon_estimation_heuristic'] = 'route-average'\n",
    "            assert param is not None\n",
    "            payload['network_hop_carbon_estimation_route_average_ratio_threshold'] = \\\n",
    "                param\n",
    "        elif heuristic == 'nearest-neighbor':\n",
    "            payload['network_hop_carbon_estimation_heuristic'] = 'nearest-neighbor'\n",
    "            assert param is not None\n",
    "            payload['network_hop_carbon_estimation_distance_km_threshold'] = param\n",
    "        else:\n",
    "            payload['network_hop_carbon_estimation_heuristic'] = 'world-average'\n",
    "    #end\n",
    "\n",
    "def prepare_request_payloads(payload: dict,\n",
    "                             minimum_known_carbon_power_ratio: Optional[float] = None,\n",
    "                             heuristic: Optional[str] = None,\n",
    "                             param: Optional[Any] = None) -> list[dict]:\n",
    "    payloads: list[dict] = []\n",
    "    for original_location in ALL_REGION_IDS:\n",
    "        for route_source in ALL_ROUTE_SOURCES:\n",
    "            request = copy.deepcopy(payload)\n",
    "            request['original_location'] = original_location\n",
    "            request['inter_region_route_source'] = route_source\n",
    "            # Real case, no estimation\n",
    "            set_estimation_heuristic(request, False)\n",
    "            request['only_emap_full_range_isos_for_network_hops'] = False\n",
    "            payloads.append(request)\n",
    "            # Estimated case, force certain regions to have no data\n",
    "            request = copy.deepcopy(request)\n",
    "            set_estimation_heuristic(request, True,\n",
    "                                     minimum_known_carbon_power_ratio,\n",
    "                                     heuristic, param)\n",
    "            # update_payload(request, original_location, ['gcloud:asia-northeast1'])\n",
    "            request['only_emap_full_range_isos_for_network_hops'] = True\n",
    "            payloads.append(request)\n",
    "    return payloads\n",
    "\n",
    "def get_network_emission_rates(details, src_region) -> dict[tuple[str, str], pd.Series]:\n",
    "    emission_rates_by_region_pair: dict[tuple[str, str], pd.Series] = {}\n",
    "    for dst_region in details:\n",
    "        # Ignore self-to-self data\n",
    "        if src_region == dst_region:\n",
    "            continue\n",
    "        if dst_region in EXCLUDED_REGION_IDS:\n",
    "            continue\n",
    "        region_pair = (src_region, dst_region)\n",
    "        # temporary issue, need to fix na values in compute in the API\n",
    "        # ds_compute = pd.Series(details[dst_region]['emission_rates']['compute']).fillna(-1)\n",
    "        ds_network = pd.Series(details[dst_region]['emission_rates']['transfer.network'])\n",
    "        emission_rates_by_region_pair[region_pair] = ds_network\n",
    "    return emission_rates_by_region_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_error_percents(error_percents: list[float]):\n",
    "    ds_error_percent = pd.Series(error_percents)\n",
    "    REGION_PAIR_COUNT = sum(1 for src in ALL_REGION_IDS for dst in ALL_REGION_IDS if src != dst)\n",
    "    coverage = ds_error_percent.size / REGION_PAIR_COUNT\n",
    "    mean = ds_error_percent.mean()\n",
    "    percentile90 = ds_error_percent.quantile(0.9)\n",
    "    percentile99 = ds_error_percent.quantile(0.99)\n",
    "    return {\n",
    "        'coverage': coverage,\n",
    "        'mean': mean,\n",
    "        '90th': percentile90,\n",
    "        '99th': percentile99,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_percent_by_region_pair(real_by_region_pair: dict[tuple[str, str], pd.Series],\n",
    "                                     estimate_by_region_pair: dict[tuple[str, str], pd.Series]) -> dict[tuple[bool, str, str], float]:\n",
    "    \"\"\"Get the error percentage for each region pair.\n",
    "    \n",
    "        Args:\n",
    "            network_emission_rates_by_only_full_range_region_pair: A dictionary of region pair to network emission rates.\n",
    "    \"\"\"\n",
    "    region_pairs = set(real_by_region_pair.keys()).intersection(set(estimate_by_region_pair.keys()))\n",
    "    error_percent_by_region_pair: dict[tuple[str, str], float] = {}\n",
    "    for region_pair in region_pairs:\n",
    "        if region_pair not in real_by_region_pair or region_pair not in estimate_by_region_pair:\n",
    "            continue\n",
    "        if region_pair[0] == region_pair[1]:\n",
    "            continue\n",
    "\n",
    "        ds_real = real_by_region_pair[region_pair]\n",
    "        ds_estimate = estimate_by_region_pair[region_pair]\n",
    "        if ds_real.empty or ds_estimate.empty:\n",
    "            print(f\"Empty dataset for {region_pair}: \")\n",
    "            continue\n",
    "        # Sometimes they are off by one index\n",
    "        if not ds_real.index.equals(ds_estimate.index):\n",
    "            ds_estimate = ds_estimate.reindex(ds_real.index)\n",
    "        assert ds_real.index.equals(ds_estimate.index), f\"Index mismatch for {region_pair}: {ds_real.index} != {ds_estimate.index}\"\n",
    "        ds_error = (ds_real - ds_estimate).abs()\n",
    "        ds_error_percent = ds_error / ds_real * 100\n",
    "        if not ds_error_percent[ds_error_percent.isnull()].empty:\n",
    "            print(f\"NaN error percent for {region_pair}\")\n",
    "            print('nan index:', ds_error_percent[ds_error_percent.isnull()].index)\n",
    "            raise ValueError(f\"NaN error percent for {region_pair}\")\n",
    "        error_percent_by_region_pair[region_pair] = ds_error_percent.to_list()\n",
    "    return error_percent_by_region_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph percentage error on estimation\n",
    "\n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "def plot_and_summarize_percent_error(error_percent_by_route_source_region_pair: dict[str, dict[tuple[str, str], float]],\n",
    "                                     figname: str = None):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    for route_source in ALL_ROUTE_SOURCES:\n",
    "        print(route_source)\n",
    "        error_percent_by_region_pair = error_percent_by_route_source_region_pair[route_source]\n",
    "        l_error_average = flatten(list(error_percent_by_region_pair.values()))\n",
    "        summary = summarize_error_percents(l_error_average)\n",
    "        for key, value in summary.items():\n",
    "            print(f\"\\t{key}: {value:.2f}\")\n",
    "\n",
    "        # Plot the error difference\n",
    "        valid_region_pair_count = len(error_percent_by_region_pair)\n",
    "        plot_cdf_array(l_error_average, label=route_source, include_count=True, override_count=valid_region_pair_count)\n",
    "\n",
    "    plt.xlabel('Estimation error (%)')\n",
    "    # curr_lim = plt.xlim()\n",
    "    # print(curr_lim)\n",
    "    plt.xlim((-5., 100.))\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.savefig(figname, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimation_heuristic_subfolder_name(min_ratio: str, heuristic: str, param: Optional[Any] = None):\n",
    "    return f'min_ratio_{min_ratio}.' + heuristic.replace('-', '_') + (f'.{param}' if param and heuristic == 'nearest-neighbor' else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_payload = copy.deepcopy(PAYLOAD_FULL_DAY_NETWORK_TRANSFER_ONLY)\n",
    "\n",
    "# base_payload['carbon_accounting_mode'] = 'compute-only'\n",
    "base_payload['schedule']['start_time'] = START_TIME.isoformat()\n",
    "base_payload['schedule']['max_delay'] = timedelta(days=DAYS_TO_COVER).total_seconds() - base_payload['runtime']\n",
    "# print(base_payload['schedule']['max_delay'])\n",
    "\n",
    "HEURISTIC_PARAM_TUPLES = [\n",
    "    (0.75, 'route-average', 0.5),\n",
    "    # (0.5, 'nearest-neighbor', 500),\n",
    "    # (0.5, 'nearest-neighbor', 1500),\n",
    "    (0.75, 'nearest-neighbor', 50000),\n",
    "    (0.75, 'world-average', None)\n",
    "]\n",
    "\n",
    "# def get_response_file_path(request: dict) -> str:\n",
    "#     original_location = request['original_location']\n",
    "#     route_source = request['inter_region_route_source']\n",
    "#     is_estimated = bool(request['only_emap_full_range_isos_for_network_hops'])\n",
    "#     filename = f'response.{original_location}.{is_estimated}.{route_source}.json'\n",
    "#     return f\"jsons/{payload['inter_region_route_source']}.{payload['original_location']}.json\"\n",
    "\n",
    "for min_ratio, heuristic, param in HEURISTIC_PARAM_TUPLES:\n",
    "    print('Heuristic:', heuristic, 'Param:', param)\n",
    "    subfolder = get_estimation_heuristic_subfolder_name(min_ratio, heuristic, param)\n",
    "    os.makedirs(f'jsons/{subfolder}', exist_ok=True)\n",
    "    payloads = prepare_request_payloads(base_payload, min_ratio, heuristic, param)\n",
    "    print(f'Added {len(payloads)} requests to the queue')\n",
    "\n",
    "    perf_start_time = time.time()\n",
    "    has_errors = False\n",
    "    response_count = 0\n",
    "    # random.shuffle(payloads)\n",
    "\n",
    "    real_network_emission_rates_by_route_source_region_pair: dict[str, dict[tuple[str, str], pd.Series]] = {}\n",
    "    estimated_network_emission_rates_by_route_source_region_pair: dict[str, dict[tuple[str, str], pd.Series]] = {}\n",
    "    for route_source in ALL_ROUTE_SOURCES:\n",
    "        real_network_emission_rates_by_route_source_region_pair[route_source] = {}\n",
    "        estimated_network_emission_rates_by_route_source_region_pair[route_source] = {}\n",
    "    with ThreadPoolExecutor(THREAD_COUNT) as pool:\n",
    "        for response, error in pool.map(_call_carbon_api_no_throw, payloads):\n",
    "            original_location = response['request']['original_location']\n",
    "            route_source = response['request']['inter_region_route_source']\n",
    "            is_estimated = bool(response['request']['only_emap_full_range_isos_for_network_hops'])\n",
    "            filename = f'response.{original_location}.{is_estimated}.{route_source}.json'\n",
    "            filepath = f\"jsons/{subfolder}/{filename}\"\n",
    "            json.dump(response, open(filepath, \"w\"), indent=4)\n",
    "            elapsed = time.time() - perf_start_time\n",
    "            response_count += 1\n",
    "            print(f'{response_count} {elapsed:.2f} {original_location} - {route_source}, {is_estimated}: {\"failure\" if error else \"success\"}', file=sys.stderr)\n",
    "            if error:\n",
    "                response[\"error\"] = error\n",
    "                has_errors = True\n",
    "                break\n",
    "            network_emission_rates = get_network_emission_rates(response['details'], original_location)\n",
    "            for region_pair, series in network_emission_rates.items():\n",
    "                if is_estimated:\n",
    "                    assert region_pair not in estimated_network_emission_rates_by_route_source_region_pair[route_source]\n",
    "                    estimated_network_emission_rates_by_route_source_region_pair[route_source][region_pair] = series\n",
    "                else:\n",
    "                    assert region_pair not in real_network_emission_rates_by_route_source_region_pair[route_source]\n",
    "                    real_network_emission_rates_by_route_source_region_pair[route_source][region_pair] = series\n",
    "    # End of ThreadPoolExecutor\n",
    "    if has_errors:\n",
    "        print('WARNING!!! There are errors in the responses. Please check the JSON files.')\n",
    "    else:\n",
    "        print('All done.')\n",
    "\n",
    "    # Process results\n",
    "    error_percent_by_route_source_region_pair: dict[str, dict[tuple[str, str], float]] = {}\n",
    "    for route_source in ALL_ROUTE_SOURCES:\n",
    "        error_percent_by_route_source_region_pair[route_source] = get_error_percent_by_region_pair(\n",
    "            real_network_emission_rates_by_route_source_region_pair[route_source],\n",
    "            estimated_network_emission_rates_by_route_source_region_pair[route_source])\n",
    "    plot_and_summarize_percent_error(error_percent_by_route_source_region_pair,\n",
    "                                     figname=f'plots/percent_error.{subfolder}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis on route carbon estimation:\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def get_transfer_power_with_iso_ratios(response) -> dict[tuple[str, str], float]:\n",
    "    \"\"\"Get the transfer power with ISO ratio on a per-region-pair basis.\n",
    "    \n",
    "        Args:\n",
    "            data: The carbon API response.\n",
    "    \"\"\"\n",
    "    ratios_by_region_pair: dict[tuple[str, str], float] = {}\n",
    "    src_region = response['request']['original_location']\n",
    "    if 'error' in response:\n",
    "        return {}\n",
    "    for dst_region, d in response['details'].items():\n",
    "        if src_region == dst_region:\n",
    "            continue\n",
    "        if src_region in EXCLUDED_REGION_IDS or dst_region in EXCLUDED_REGION_IDS:\n",
    "            continue\n",
    "        ratio = d.get('transfer.network.power_ratio_with_carbon_data', math.nan)\n",
    "        ratios_by_region_pair[(src_region, dst_region)] = ratio\n",
    "    return ratios_by_region_pair\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "region_pairs_without_this_ratio = set()\n",
    "for route_source in ALL_ROUTE_SOURCES:\n",
    "    all_transfer_power_with_iso_ratios = {}\n",
    "    for src_region in ALL_REGION_IDS:\n",
    "        subfolder = get_estimation_heuristic_subfolder_name(0, 'world-average')\n",
    "        data = json.load(open(f\"jsons/{subfolder}/response-{src_region}.True.{route_source}.json\"))\n",
    "        transfer_power_with_iso_ratios = get_transfer_power_with_iso_ratios(data)\n",
    "        # for t in transfer_power_with_iso_ratios.keys():\n",
    "        #     if t[0] == t[1]:\n",
    "        #         print(t)\n",
    "        regions_without_this_ratio = set()\n",
    "        if 'warnings' in data:\n",
    "            regions_without_this_ratio.update(data['warnings'].keys())\n",
    "        if 'error' in data:\n",
    "            regions_without_this_ratio.update(data['details'].keys())\n",
    "        regions_without_this_ratio.add(src_region)\n",
    "        expected_regions = set(ALL_REGION_IDS).difference(regions_without_this_ratio)\n",
    "        actual_regions = set(t[1] for t in transfer_power_with_iso_ratios.keys())\n",
    "\n",
    "        # print(original_location, len(actual_regions),\n",
    "        #       len(data['warnings']) if 'warnings' in data else 0,\n",
    "        #       len(data['details']) if 'error' in data else 0)\n",
    "\n",
    "        assert actual_regions == expected_regions, f\"Missing region pair for {src_region} - {route_source}. Expected: {expected_regions}, Actual: {actual_regions}, difference: {expected_regions.difference(actual_regions)}\"\n",
    "\n",
    "        region_pairs_without_this_ratio.update((src_region, dst) for dst in regions_without_this_ratio \\\n",
    "                                               if src_region != dst)\n",
    "        all_transfer_power_with_iso_ratios.update(transfer_power_with_iso_ratios)\n",
    "    plot_cdf_array(list(all_transfer_power_with_iso_ratios.values()), label=route_source, include_count=True)\n",
    "    print(route_source, len(all_transfer_power_with_iso_ratios))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.xlabel(\"Ratio of network power with carbon data\")\n",
    "# plt.title(f\"Ratio of transfer power with carbon data\")\n",
    "plt.savefig('network_power_with_carbon_data.ratio.png')\n",
    "\n",
    "# unique_regions = set(p[0] for p in region_pairs_without_this_ratio).union(\n",
    "#     set(p[1] for p in region_pairs_without_this_ratio))\n",
    "# print('unique regions:', len(unique_regions))\n",
    "# for region in unique_regions:\n",
    "#     print(region)\n",
    "# print()\n",
    "\n",
    "# print('region pairs without ratio:', len(region_pairs_without_this_ratio))\n",
    "# for region_pair in region_pairs_without_this_ratio:\n",
    "#     print(region_pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for route_source in ALL_ROUTE_SOURCES[:1]:\n",
    "    all_affected_region_pairs = set()\n",
    "    all_isos_to_region_pairs = defaultdict(set)\n",
    "    for src_region in ALL_REGION_IDS:\n",
    "        subfolder = get_estimation_heuristic_subfolder_name(0, 'world-average')\n",
    "        data = json.load(open(f\"jsons/{subfolder}/response-{src_region}.True.{route_source}.json\"))\n",
    "        isos_to_region_pairs = get_iso_to_affected_region_pairs(data)\n",
    "        for iso, region_pairs in isos_to_region_pairs.items():\n",
    "            all_isos_to_region_pairs[iso].update(region_pairs)\n",
    "    isos = list(all_isos_to_region_pairs.keys())\n",
    "    print(f\"Found {len(isos)} unique ISOs under {route_source}: {isos}\")\n",
    "\n",
    "    for iso in all_isos_to_region_pairs:\n",
    "        if iso in emap_zone_ids_with_full_2023_data:\n",
    "            continue\n",
    "        # print(f\"Warning: {iso} is not in the list of EMAP zone IDs with full 2023 data. \"\n",
    "        #         f\"Affected regions: {all_isos_to_region_pairs[iso]}\")\n",
    "        all_affected_region_pairs.update(all_isos_to_region_pairs[iso])\n",
    "    print(f\"Found {len(all_affected_region_pairs)} unique affected region pairs under {route_source}:\")\n",
    "    print(all_affected_region_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile individual workloads\n",
    "\n",
    "# Load payload from above\n",
    "assert payload, \"Error: payload is not defined\"\n",
    "\n",
    "# Restrict to two locations\n",
    "if 'candidate_providers' in payload:\n",
    "    del payload['candidate_providers']\n",
    "payload['candidate_locations'] = [\n",
    "    {\n",
    "        \"id\": \"AWS:us-east-1\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"AWS:us-west-1\"\n",
    "    },\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class Workload:\n",
    "    name: str\n",
    "    core_count: int\n",
    "    runtime_s: float\n",
    "    input_gb: float\n",
    "    output_gb: float\n",
    "\n",
    "\"\"\"csv\n",
    "name,core_count,runtime_s,input_gb,output_gb\n",
    "compression (gzip),13,2.1,1.2,0.19\n",
    "compression (bzip),31,3.2,1.2,0.142\n",
    "code compilation (linux v5.16),40,391,1.2,0.195\n",
    "video resize (4k->1080p),4,261,4.6,0.251\n",
    "video effect (4k, h.264),8,307,1.2,0.122\n",
    "video effect (4k, h.265),8,307,0.15,0.117\n",
    "ML inference (resnet50, 1k images),1,246,0.154,0\n",
    "\"\"\"\n",
    "\n",
    "profiled_workloads = [\n",
    "    Workload(\"compression (gzip)\", 13, 2.1, 1.2, 0.19),\n",
    "    Workload(\"compression (bzip)\", 31, 3.2, 1.2, 0.142),\n",
    "    Workload(\"code compilation (linux v5.16)\", 40, 391, 1.2, 0.195),\n",
    "    Workload(\"video resize (4k->1080p)\", 4, 261, 4.6, 0.251),\n",
    "    Workload(\"video effect (4k, h.264)\", 8, 307, 1.2, 0.122),\n",
    "    Workload(\"video effect (4k, h.265)\", 8, 307, 0.15, 0.117),\n",
    "    Workload(\"ML inference (resnet50, 1k images)\", 1, 246, 0.154, 0),\n",
    "]\n",
    "\n",
    "CARBON_API_URL = 'http://yak-03.sysnet.ucsd.edu/carbon-aware-scheduler/'\n",
    "\n",
    "for workload in profiled_workloads:\n",
    "    workload.runtime_s *= 10\n",
    "    workload.input_gb *= 10\n",
    "    workload.output_gb *= 10\n",
    "    payload['runtime'] = workload.runtime_s\n",
    "    payload['schedule']['max_delay'] = timedelta(days=1).total_seconds() - workload.runtime_s\n",
    "    payload['dataset']['input_size_gb'] = workload.input_gb\n",
    "    payload['dataset']['output_size_gb'] = workload.output_gb\n",
    "    payload['core_count'] = workload.core_count\n",
    "    print(f'{workload.name} - {workload.core_count}c * {workload.runtime_s}s', f'{workload.input_gb}G/{workload.output_gb}G')\n",
    "    print('Core-hours/GB:', workload.core_count * workload.runtime_s / timedelta(hours=1).total_seconds() / (workload.input_gb + workload.output_gb))\n",
    "\n",
    "    response = requests.get(CARBON_API_URL, json=payload)\n",
    "    assert response.ok, f\"Error: API call failed with status code {response.status_code}: {response.text}\"\n",
    "    data = response.json(parse_float=lambda s: float('%.6g' % float(s)))\n",
    "    # print(json.dumps(data, indent=4))\n",
    "    plot_carbon_api_response(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average/min/max transfer carbon cost across all region pairs\n",
    "\n",
    "# TODO: check this code from copilot\n",
    "def get_carbon_cost_for_region_pair(source_region: str, destination_region: str, payload: dict) -> dict[time, float]:\n",
    "    \"\"\"Calculate the carbon cost for a region pair.\n",
    "    \n",
    "        Args:\n",
    "            source_region: The source region.\n",
    "            destination_region: The destination region.\n",
    "            payload: The payload to be sent to the API.\n",
    "    \"\"\"\n",
    "    payload['candidate_locations'] = [\n",
    "        {\n",
    "            \"id\": source_region\n",
    "        },\n",
    "        {\n",
    "            \"id\": destination_region\n",
    "        },\n",
    "    ]\n",
    "    response = requests.get(CARBON_API_URL, json=payload)\n",
    "    assert response.ok, f\"Error: API call failed with status code {response.status_code}: {response.text}\"\n",
    "    data = response.json(parse_float=lambda s: float('%.6g' % float(s)))\n",
    "    carbon_emissions_transfer = data['raw-scores'][destination_region]['carbon-emission-from-migration']\n",
    "    return carbon_emissions_transfer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ad-hoc: verify that new optimization is no worse than the old one, by sweeping over all parameters\n",
    "\n",
    "payload = {\n",
    "    \"runtime\": 5400,\n",
    "    \"schedule\": {\n",
    "        \"type\": \"onetime\",\n",
    "        \"start_time\": \"2023-05-24T22:00:00-05:00\",\n",
    "        \"max_delay\": 36000\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"input_size_gb\": 20,\n",
    "        \"output_size_gb\": 12\n",
    "    },\n",
    "    \"candidate_locations\": [\n",
    "        {\n",
    "            \"id\": \"AWS:us-east-1\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"AWS:us-west-1\"\n",
    "        },\n",
    "        # {\n",
    "        #     \"id\": \"AWS:us-west-1\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"id\": \"AWS:us-west-1\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"id\": \"AWS:us-west-1\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"id\": \"AWS:us-west-1\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"id\": \"AWS:us-west-1\"\n",
    "        # },\n",
    "    ],\n",
    "    \"use_prediction\": False,\n",
    "    \"carbon_data_source\": \"emap\",\n",
    "    \"watts_per_core\": 5,\n",
    "    \"core_count\": 40,\n",
    "    \"original_location\": \"AWS:us-east-1\",\n",
    "    \"carbon_accounting_mode\": \"compute-and-network\",\n",
    "    \"optimize_carbon\": True,\n",
    "}\n",
    "\n",
    "\n",
    "def compare_results(payload1: dict, payload2: dict):\n",
    "    response1 = requests.get(CARBON_API_URL, json=payload1)\n",
    "    response2 = requests.get(CARBON_API_URL, json=payload2)\n",
    "    if response1.ok != response2.ok:\n",
    "        print(\"Inconsistent response status\")\n",
    "        print(payload)\n",
    "        print(response1)\n",
    "        print(response2)\n",
    "        return\n",
    "    elif response1.ok == False:\n",
    "        print(\"Both responses are not ok\")\n",
    "        print(payload)\n",
    "        print(response1.status_code)\n",
    "        print(response1.json())\n",
    "        return\n",
    "    parse_float=lambda s: float('%.9g' % float(s))\n",
    "    json1 = response1.json(parse_float=parse_float)\n",
    "    json2 = response2.json(parse_float=parse_float)\n",
    "    del json1['request']\n",
    "    del json2['request']\n",
    "    if json1 != json2:\n",
    "        should_report = False\n",
    "        for region in json2['raw-scores']:\n",
    "            if json2['raw-scores'][region]['carbon-emission'] > json1['raw-scores'][region]['carbon-emission']:\n",
    "                print(f\"Worse result for {region}\")\n",
    "                should_report = True\n",
    "        if should_report:\n",
    "            print(\"Inconsistent response content\")\n",
    "            print(payload)\n",
    "            print(json1)\n",
    "            print(json2)\n",
    "            return\n",
    "    else:\n",
    "        print(\"Same result\")\n",
    "    print(json1)\n",
    "    print(json2)\n",
    "    print('old:', end='')\n",
    "    for region in json1['raw-scores']:\n",
    "        print('\\t', region, json1['raw-scores'][region]['carbon-emission'], end='')\n",
    "    print()\n",
    "    print('new:', end='')\n",
    "    for region in json2['raw-scores']:\n",
    "        print('\\t', region, json2['raw-scores'][region]['carbon-emission'], end='')\n",
    "    print()\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "CARBON_API_URL='http://yak-03.sysnet.ucsd.edu/carbon-aware-scheduler/'\n",
    "\n",
    "# # Fuzzing errorneous input\n",
    "# payloads = [\n",
    "#     {'runtime': 14802, 'schedule': {'type': 'onetime', 'start_time': '2022-08-18T12:18:57+00:00', 'max_delay': 36000}, 'dataset': {'input_size_gb': 20, 'output_size_gb': 69}, 'candidate_locations': [{'id': 'AWS:us-east-1'}, {'id': 'AWS:us-west-1'}], 'use_prediction': False, 'carbon_data_source': 'emap', 'watts_per_core': 5, 'core_count': 5, 'original_location': 'AWS:us-east-1', 'carbon_accounting_mode': 'compute-and-network', 'optimize_carbon': True, 'use_new_optimization': False},\n",
    "# ]\n",
    "\n",
    "# for payload in payloads:\n",
    "#     payload_new_optimization = copy.deepcopy(payload)\n",
    "#     payload_new_optimization['use_new_optimization'] = True\n",
    "#     compare_results(payload, payload_new_optimization)\n",
    "\n",
    "# Make the API call\n",
    "for i in range(365):\n",
    "    for runtime_seconds in [random.randint(1, 3600 * 24) for _ in range(1)]:\n",
    "        START_TIME = datetime(2022, 1, 1, 0, 0, 0, tzinfo=timezone.utc) + timedelta(days=i)\n",
    "        START_TIME += timedelta(seconds=random.randint(0, 3600 * 24))\n",
    "        data_input_size_gb = random.randint(1, 100)\n",
    "        data_output_size_gb = random.randint(1, 100)\n",
    "        core_count = random.randint(1, 100)\n",
    "\n",
    "        payload['schedule']['start_time'] = START_TIME.isoformat()\n",
    "        payload['runtime'] = runtime_seconds\n",
    "        payload['dataset']['input_size_gb'] = data_input_size_gb\n",
    "        payload['dataset']['output_size_gb'] = data_output_size_gb\n",
    "        payload['core_count'] = core_count\n",
    "        payload['use_new_optimization'] = False\n",
    "        print(START_TIME, f'{core_count}c * {runtime_seconds}s', f'{data_input_size_gb}G/{data_output_size_gb}G')\n",
    "\n",
    "        payload_new_optimization = copy.deepcopy(payload)\n",
    "        payload_new_optimization['use_new_optimization'] = True\n",
    "        compare_results(payload, payload_new_optimization)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
