{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "# import requests_cache\n",
    "from dataclasses import dataclass, asdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import random\n",
    "import math\n",
    "import bisect\n",
    "import arrow\n",
    "import pytz\n",
    "from datetime import datetime, date, timedelta\n",
    "from timezonefinder import TimezoneFinder\n",
    "from matplotlib import pyplot as plt, dates\n",
    "from matplotlib.ticker import *\n",
    "from carbon_api_client import *\n",
    "from matplotlib_helper import *\n",
    "from typing import List, Any\n",
    "from dateutil import tz\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_utc_time_of_day = True\n",
    "enable_savefig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests_cache.install_cache('http_cache', backend='filesystem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Statistics(Enum):\n",
    "    MEAN = 'mean'\n",
    "    MEDIAN = 'median'\n",
    "    STD = 'std'\n",
    "    MIN = 'min'\n",
    "    MAX = 'max'\n",
    "    SIZE = 'size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_down(dt: datetime, round_to: timedelta) -> datetime:\n",
    "    \"\"\"Round down the given datetime to the specified interval.\"\"\"\n",
    "    # datetime.min has tzinfo=None\n",
    "    total_seconds = (dt.replace(tzinfo=None, microsecond=0) - datetime.min).total_seconds()\n",
    "    remainder_seconds = total_seconds % round_to.total_seconds()\n",
    "    dt = dt.replace(microsecond=0)\n",
    "    return dt - timedelta(seconds=remainder_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "M_PUBLIC_CLOUD_LOCATION = {\n",
    "    ('AWS', 'us-west-1'): (37.00578, -121.56828),\n",
    "    ('AWS', 'us-west-2'): (45.840410, -119.289460),\n",
    "    ('AWS', 'us-east-1'): (39.983334, -82.983330),\n",
    "    ('AWS', 'us-east-2'): (39.040283, -77.485165),\n",
    "    # Not needed as Azure carbon API relies on region name instead of geocoordinates\n",
    "    # Note: below are Azure reigons, but we want to use 'AWS' to call into c3lab API\n",
    "    ('Azure', 'eastus'):            (37.3719, -79.8164),\n",
    "    ('Azure', 'eastus2'):           (36.6681, -78.3889),\n",
    "    ('Azure', 'southcentralus'):    (29.4167, -98.5),\n",
    "    ('Azure', 'westus2'):           (47.233, -119.852),\n",
    "    ('Azure', 'westus3'):           (33.448376, -112.074036),\n",
    "    ('Azure', 'centralus'):         (41.5908, -93.6208),\n",
    "    ('Azure', 'eastus2euap'):       (36.6681, -78.3889),\n",
    "    ('Azure', 'northcentralus'):    (41.8819, -87.6278),\n",
    "    ('Azure', 'westus'):            (37.783, -122.417),\n",
    "    ('Azure', 'centraluseuap'):     (41.5908, -93.6208),\n",
    "    ('Azure', 'westcentralus'):     (40.890, -110.234),\n",
    "    ('SPP', 'kansus'):              (39.071971, -94.663875),\n",
    "    ('NY', 'syracuse'):             (43.0415, -76.14),\n",
    "    ('PACW', 'modford'):            (42.3265, -122.8756),\n",
    "    ('BPA', 'eugene'):              (44.0521, -123.0868),\n",
    "}\n",
    "def get_location_for_public_cloud(cloud_vendor, region):\n",
    "    '''Looks up the GPS coordinate for public cloud region.'''\n",
    "    if (cloud_vendor, region) in M_PUBLIC_CLOUD_LOCATION:\n",
    "        return M_PUBLIC_CLOUD_LOCATION[(cloud_vendor, region)]\n",
    "    else:\n",
    "        return (math.nan, math.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US/Canada\n",
    "azure_regions_americas = [\n",
    "    'westus',\n",
    "    'westus2',\n",
    "    'westcentralus',\n",
    "    'westus3',\n",
    "    'eastus',\n",
    "    'centralus',\n",
    "    'southcentralus',\n",
    "    # 'canadacentral',\n",
    "    # 'canadaeast',\n",
    "]\n",
    "\n",
    "# Europe\n",
    "azure_regions_europe = [\n",
    "    'uksouth',\n",
    "    'francecentral',\n",
    "    'germanywestcentral',\n",
    "    'northeurope',\n",
    "    'norwayeast',\n",
    "    'swedencentral',\n",
    "    'westeurope',\n",
    "]\n",
    "\n",
    "# Australia\n",
    "azure_regions_aus = [\n",
    "    'australiaeast',\n",
    "    'australiasoutheast',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_azure_regions_to_isos = {\n",
    "    # US/Canada\n",
    "    'westus': 'CAISO_NORTH',\n",
    "    'westus2': 'GCPD',\n",
    "    'westcentralus': 'PACE',\n",
    "    'westus3': 'AZPS',\n",
    "    'eastus': 'PJM_ROANOKE',\n",
    "    'centralus': 'MISO_MASON_CITY',\n",
    "    'southcentralus': 'ERCOT_SANANTONIO',\n",
    "    'canadacentral': 'IESO_NORTH',\n",
    "    'canadaeast': 'HQ',\n",
    "    # Europe\n",
    "    'uksouth': 'UK',\n",
    "    'francecentral': 'FR',\n",
    "    'germanywestcentral': 'DE',\n",
    "    'northeurope': 'IE',\n",
    "    'norwayeast': 'NO',\n",
    "    'swedencentral': 'SE',\n",
    "    'westeurope': 'NL',\n",
    "    # Australia\n",
    "    'australiaeast': 'NEM_NSW',\n",
    "    'australiasoutheast': 'NEM_VIC',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# run_count = 0\n",
    "\n",
    "def plot_timeseries(data_array, plot_axis=None, timestamp_column_name='timestamp', prefix=None, use_relative_time=False, color=None, index=0):\n",
    "    # global run_count\n",
    "    data_array = [asdict(entry) for entry in data_array]\n",
    "    x = [entry[timestamp_column_name] for entry in data_array]\n",
    "\n",
    "    # timestamp_deltas = np.diff(x)\n",
    "    # values, counts = np.unique(timestamp_deltas, return_counts=True)\n",
    "    # print(values, counts)\n",
    "\n",
    "    if use_relative_time:\n",
    "        start_time = x[0]\n",
    "        x = [(t - start_time).total_seconds() for t in x]\n",
    "    data_keys = []\n",
    "    for key in data_array[0].keys():\n",
    "        if key == timestamp_column_name:\n",
    "            continue\n",
    "        data_keys.append(key)\n",
    "    lines = []\n",
    "    for key in data_keys:\n",
    "        data_series = [entry[key] for entry in data_array]\n",
    "        label = (('%s - ' % prefix if prefix else '') + key) if len(data_keys) > 1 else (prefix if prefix else '')\n",
    "        if plot_axis is None:\n",
    "            plot_axis = plt.gca()\n",
    "        line = plot_axis.plot(x, data_series, color=color, linestyle=get_linestyle(index), label=label, marker=None)\n",
    "        # if run_count == 0:\n",
    "        #     plot_axis.fill_between(x, y1=data_series, where=[True if len(x)*17.5/24 < i < len(x)*20.5/24 else False for i in range(len(x))], alpha=0.2)\n",
    "        #     run_count += 1\n",
    "        tzinfo = x[0].tzinfo\n",
    "        plot_axis.xaxis_date(tz=tzinfo)\n",
    "        index += 1\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pd_timeseries(df: pd.DataFrame, errors=None, plot_axis=None, label=\"\", use_relative_time=False, color=None, index=0):\n",
    "    # x = [ts.to_pydatetime() for ts in df.index.tolist()]\n",
    "    # y = df.values\n",
    "    # print('df columns:', df.columns.values)\n",
    "    # print('df:', df)\n",
    "    # print('df.values:', df.values)\n",
    "    # print('errors.values:', errors.values)\n",
    "    if use_relative_time:\n",
    "        raise NotImplementedError()\n",
    "        # start_time = x[0]\n",
    "        # x = [(t - start_time).total_seconds() for t in x]\n",
    "    if plot_axis is None:\n",
    "        plot_axis = plt.gca()\n",
    "    # line = plot_axis.plot(x, y, color=color, linestyle=get_linestyle(index), label=label, marker=None)\n",
    "    # tzinfo = df.index[0].tzinfo\n",
    "    # plot_axis.xaxis_date(tz=tzinfo)\n",
    "    if color is None: color = get_next_color()\n",
    "    column = df.columns.values.tolist()[0]\n",
    "    if errors is not None:\n",
    "        line = df.plot(ax=plot_axis, yerr=errors, x_compat=True, color=color, linestyle=get_linestyle(index), label=label, capsize=2)\n",
    "    else:\n",
    "        line = df.plot(ax=plot_axis, x_compat=True, color=color, linestyle=get_linestyle(index), label=label)\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gsf_carbon_api_prediction(cloud_vendor: str, region: str, start: arrow.Arrow = None, end: arrow.Arrow = None) -> CarbonIntensityData:\n",
    "    url_get_carbon_intensity = 'https://carbon-aware-api.azurewebsites.net/emissions/forecasts/current'\n",
    "    response = requests.get(url_get_carbon_intensity, params={\n",
    "        'location': [region],\n",
    "        'dataStartAt': start,\n",
    "        'dataEndAt': end.shift(minutes=-5),\n",
    "        'windowsSize': 5,\n",
    "    })\n",
    "    assert response.ok, \"GSF carbon intensity prediction lookup failed (%d): %s\" % (response.status_code, response.text)\n",
    "    response_json = response.json()\n",
    "\n",
    "    print(len(response_json))\n",
    "    assert len(response_json) == 1\n",
    "\n",
    "    response_element = response_json[0]\n",
    "    generatedAt = response_element['generatedAt']\n",
    "\n",
    "    locations = set()\n",
    "    timeseries = []\n",
    "    for entry in response_element['forecastData']:\n",
    "        locations.add(entry['location'])\n",
    "        timestamp = arrow.get(entry['timestamp']).datetime\n",
    "        carbon_intensity = float(entry['value'])\n",
    "        duration = timedelta(minutes=entry['duration'])\n",
    "        timeseries.append(TimestampdValue(timestamp, carbon_intensity))\n",
    "    ds = create_pd_series([e.timestamp for e in timeseries], [e.value for e in timeseries])\n",
    "    iso = ','.join(locations)\n",
    "\n",
    "    return CarbonIntensityData(cloud_vendor, region, iso, timeseries, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def call_gsf_carbon_api(cloud_vendor: str, region: str, start: arrow.Arrow, end: arrow.Arrow, get_prediction=False) -> CarbonIntensityData:\n",
    "    if get_prediction:\n",
    "        return call_gsf_carbon_api_prediction(cloud_vendor, region, start, end)\n",
    "\n",
    "    url_get_carbon_intensity = 'https://carbon-aware-api.azurewebsites.net/emissions/bylocations'\n",
    "    response = requests.get(url_get_carbon_intensity, params={\n",
    "        'location': [region],\n",
    "        'time': start,\n",
    "        'toTime': end,\n",
    "    })\n",
    "    assert response.ok, \"GSF carbon intensity lookup failed (%d): %s\" % (response.status_code, response.text)\n",
    "    response_json = response.json()\n",
    "\n",
    "    locations = set()\n",
    "    timeseries = []\n",
    "    for entry in response_json:\n",
    "        locations.add(entry['location'])\n",
    "        timestamp = arrow.get(entry['time']).datetime\n",
    "        carbon_intensity = float(entry['rating']) / 2.2 # lb/MWh -> g/kWh\n",
    "        duration = entry['duration']\n",
    "        timeseries.append(TimestampdValue(timestamp, carbon_intensity))\n",
    "    ds = create_pd_series([e.timestamp for e in timeseries], [e.value for e in timeseries])\n",
    "    iso = ','.join(locations)\n",
    "\n",
    "    return CarbonIntensityData(cloud_vendor, region, iso, timeseries, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_carbon_intensity_data(cloud_vendor, region, date:date = None, timerange:timedelta = timedelta(weeks=1),\n",
    "                              use_utc_time_of_day = True, get_prediction=False,\n",
    "                              desired_renewable_ratio: float = None) -> CarbonIntensityData:\n",
    "    print(cloud_vendor, region)\n",
    "    (latitude, longitude) = get_location_for_public_cloud(cloud_vendor, region)\n",
    "    if date is None:\n",
    "        date = arrow.get().shift(weeks=-1).date()\n",
    "    if use_utc_time_of_day:\n",
    "        timezone = pytz.UTC\n",
    "    else:\n",
    "        timezone_str = TimezoneFinder().timezone_at(lng=longitude, lat=latitude)\n",
    "        timezone = pytz.timezone(timezone_str)\n",
    "    date = arrow.get(date, tzinfo=timezone)\n",
    "    # print(timezone_str, date, file=sys.stderr)\n",
    "    # NOTE: temporary override _debug_\n",
    "    # if cloud_vendor == 'AWS':\n",
    "    if True:\n",
    "        assert get_prediction is False, 'No prediction support for AWS regions yet'\n",
    "        (latitude, longitude) = get_location_for_public_cloud(cloud_vendor, region)\n",
    "        ci_data = call_sysnet_carbon_intensity_api(latitude, longitude, date, date + timerange, desired_renewable_ratio=desired_renewable_ratio)\n",
    "        if ci_data is None:\n",
    "            return None\n",
    "        ci_data.cloud_vendor = cloud_vendor\n",
    "        ci_data.region = region\n",
    "        return ci_data\n",
    "    elif cloud_vendor == 'Azure':\n",
    "        return call_gsf_carbon_api(cloud_vendor, region, date, date.shift(minutes=-1) + timerange, get_prediction=get_prediction)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported region {cloud_vendor}:{region}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_carbon_intensity_stats(l_time_series: List[TimestampdValue]):\n",
    "    l_carbon_intensity = [e.value for e in l_time_series]\n",
    "    print('Avg/Min/Max carbon intensity: %.2f/%.2f/%.2f' % (\n",
    "        np.mean(l_carbon_intensity),\n",
    "        np.min(l_carbon_intensity),\n",
    "        np.max(l_carbon_intensity),\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_overlap_diff_of_carbon_intensities(time_series_1: pd.Series, time_series_2: pd.Series) -> List[float]:\n",
    "    s1_timestamps = [e.timestamp for e in time_series_1]\n",
    "    s2_timestamps = [e.timestamp for e in time_series_2]\n",
    "    union_timestamps = sorted(list(set(s1_timestamps).union(s2_timestamps)))\n",
    "    # Same index as common_timestamps\n",
    "    l1_carbon_intensity = []\n",
    "    l2_carbon_intensity = []\n",
    "    l_diff_carbon_intensity = []\n",
    "    for index in range(len(union_timestamps)):\n",
    "        curr_timestamp = union_timestamps[index]\n",
    "        if curr_timestamp in s1_timestamps:\n",
    "            index1 = s1_timestamps.index(curr_timestamp)\n",
    "        else:   # Find the previous timestamp and use that\n",
    "            index1 = max(bisect.bisect(s1_timestamps, curr_timestamp) - 1, 0)\n",
    "        if curr_timestamp in s2_timestamps:\n",
    "            index2 = s2_timestamps.index(curr_timestamp)\n",
    "        else:\n",
    "            index2 = max(bisect.bisect(s2_timestamps, curr_timestamp) - 1, 0)\n",
    "        carbon_intensity1 = time_series_1[index1].value\n",
    "        carbon_intensity2 = time_series_2[index2].value\n",
    "        l1_carbon_intensity.append(carbon_intensity1)\n",
    "        l2_carbon_intensity.append(carbon_intensity2)\n",
    "        l_diff_carbon_intensity.append(carbon_intensity2 - carbon_intensity1)\n",
    "    return l_diff_carbon_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_overlap_interval_of_carbon_intensities(time_series_1: List[TimestampdValue], time_series_2: List[TimestampdValue]) -> \\\n",
    "        List[tuple[datetime, datetime]]:\n",
    "    \"\"\"Find the intervals where carbon intensity of the first time series drops below the second.\"\"\"\n",
    "    print(\"Finding overlap in intervals\")\n",
    "    s1_timestamps = sorted([e.timestamp for e in time_series_1])\n",
    "    s2_timestamps = sorted([e.timestamp for e in time_series_2])\n",
    "    union_timestamps = sorted(list(set(s1_timestamps).union(s2_timestamps)))\n",
    "    # Same index as common_timestamps\n",
    "    l1_carbon_intensity: List[float] = []\n",
    "    l2_carbon_intensity: List[float] = []\n",
    "    overlap_intervals: List[tuple[datetime, datetime]] = []\n",
    "    interval_start_index = None\n",
    "    for index in range(len(union_timestamps)):\n",
    "        curr_timestamp = union_timestamps[index]\n",
    "        index1 = max(bisect.bisect(s1_timestamps, curr_timestamp) - 1, 0)\n",
    "        index2 = max(bisect.bisect(s2_timestamps, curr_timestamp) - 1, 0)\n",
    "        carbon_intensity1 = time_series_1[index1].value\n",
    "        carbon_intensity2 = time_series_2[index2].value\n",
    "        l1_carbon_intensity.append(carbon_intensity1)\n",
    "        l2_carbon_intensity.append(carbon_intensity2)\n",
    "        if carbon_intensity1 <= carbon_intensity2:\n",
    "            if interval_start_index is None:\n",
    "                interval_start_index = index\n",
    "        else:\n",
    "            if interval_start_index is not None:\n",
    "                timestamp_start = union_timestamps[interval_start_index]\n",
    "                timestamp_end = union_timestamps[index]\n",
    "                overlap_intervals.append((timestamp_start, timestamp_end))\n",
    "                interval_start_index = None\n",
    "    print(\"done\")\n",
    "    return overlap_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_overlap_interval_cdf(overlap_intervals: List[tuple[datetime, datetime]], label: str) -> None:\n",
    "    interval_deltas = [(interval[1] - interval[0]) for interval in overlap_intervals]\n",
    "    interval_in_hours = [delta.total_seconds() / timedelta(hours=1).total_seconds() for delta in interval_deltas]\n",
    "    plot_cdf_array(interval_in_hours, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def format_cloud_region_name(cloud_region: tuple[str, str], iso: str) -> str:\n",
    "    \"\"\"Format the name for a cloud region, including its electricity-sourcing ISO.\"\"\"\n",
    "    return f'{cloud_region[0]} {cloud_region[1]} ({iso})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pull_carbon_intensity_data(cloud_vendor_and_regions: list[tuple], start_date: datetime, end_date: datetime,\n",
    "                               get_prediction=False,\n",
    "                               desired_renewable_ratio: float = None):\n",
    "    print(f'Pulling carbon intensity data in range [{arrow.get(start_date)}, {arrow.get(end_date)}]')\n",
    "    window_size = end_date - start_date\n",
    "    all_region_time_series_data = {}\n",
    "    for (cloud_vendor, region) in cloud_vendor_and_regions:\n",
    "        carbon_intensity_data = get_carbon_intensity_data(cloud_vendor, region, date=start_date, timerange=window_size,\n",
    "                                                          use_utc_time_of_day=use_utc_time_of_day,\n",
    "                                                          get_prediction=get_prediction,\n",
    "                                                          desired_renewable_ratio=desired_renewable_ratio)\n",
    "        if not carbon_intensity_data:\n",
    "            continue\n",
    "        carbon_intensity_data.set_timeseries_interval('15min')\n",
    "\n",
    "        all_region_time_series_data[(cloud_vendor, region)] = carbon_intensity_data\n",
    "        time_series_data = carbon_intensity_data.timeseries\n",
    "        print_carbon_intensity_stats(time_series_data)\n",
    "    return all_region_time_series_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_carbon_intensity_time_series(all_region_time_series_data: dict[tuple, CarbonIntensityData], start_date: datetime, end_date: datetime, aggregate_by: timedelta = None, errorbar: bool = False):\n",
    "    # plt.figure(figsize=(8, 4))\n",
    "    plt.figure(figsize=(12, 4.8))\n",
    "    # if aggregate_by is None:\n",
    "    #     plt.figure(figsize=(12, 4.8))\n",
    "    #     fig, axes = plt.subplots(1, 1)\n",
    "    # else:\n",
    "    #     # plt.figure(figsize=(12, 12))\n",
    "    #     fig, axes = plt.subplots(3, 1)\n",
    "    for (cloud_vendor, region) in all_region_time_series_data:\n",
    "        carbon_intensity_data = all_region_time_series_data[(cloud_vendor, region)]\n",
    "        prefix=f'{cloud_vendor} {region} ({carbon_intensity_data.iso})'\n",
    "        print(f'Region: {prefix}')\n",
    "        time_series_data = carbon_intensity_data.timeseries_pd\n",
    "        print('Available data range: ', time_series_data.index.min(), time_series_data.index.max())\n",
    "        # sampled_data = sorted(random.sample(time_series_data, min(len(time_series_data), 1000)), key=lambda e: e['timestamp'])\n",
    "        sampled_data = time_series_data.loc[start_date:end_date]\n",
    "        df = sampled_data.to_frame(prefix)\n",
    "        if aggregate_by is None:\n",
    "            plot_pd_timeseries(df, use_relative_time=False)\n",
    "            # plot_timeseries(sampled_data, use_relative_time=False, prefix=prefix)\n",
    "        else:\n",
    "            # df.assign(time=df.index.time).groupby('time', as_index=True).mean()\n",
    "            modulo_datetime_by_period = lambda dt, period: (dt - datetime.min.replace(tzinfo=tz.UTC)) % period\n",
    "            grouped = df.assign(time=lambda x: modulo_datetime_by_period(x.index.to_pydatetime(), aggregate_by)).groupby('time', as_index=True)\n",
    "            means = grouped.mean()\n",
    "            errors = grouped.std()\n",
    "            mins = grouped.min()\n",
    "            maxs = grouped.max()\n",
    "            # print('means.index:', means.index.size)\n",
    "            if errorbar:\n",
    "                plot_pd_timeseries(means, errors=errors)\n",
    "            else:\n",
    "                plot_pd_timeseries(means)\n",
    "            # plt.sca(axes[0])\n",
    "            # plot_pd_timeseries(means, errors=errors)\n",
    "            # plt.sca(axes[1])\n",
    "            # plot_pd_timeseries(mins)\n",
    "            # plt.sca(axes[2])\n",
    "            # plot_pd_timeseries(maxs)\n",
    "    window_size = end_date - start_date\n",
    "    ax = plt.gca()\n",
    "    if aggregate_by is not None:\n",
    "        xlabel = 'Time in period'\n",
    "        ax.xaxis.set_major_formatter(FuncFormatter(lambda x, _: str(x/means.index.size * aggregate_by)))\n",
    "        # plt.xlim(0, means.index.size)\n",
    "    else:\n",
    "        if window_size.total_seconds() == timedelta(days=1).total_seconds():\n",
    "            date_formatter_string = \"%H:%M\"\n",
    "            xlabel = f'Time of day ({\"UTC\" if use_utc_time_of_day else \"local\"})'\n",
    "        else:\n",
    "            date_formatter_string = \"%Y/%m/%d\"\n",
    "            xlabel = 'Date'\n",
    "        ax.xaxis.set_major_formatter(dates.DateFormatter(date_formatter_string))\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Carbon intensity (gCO2/kWh)')\n",
    "    # plt.ylabel('Renewable percentage')  # _debug_\n",
    "    title_metric = 'carbon intensity'\n",
    "    # title_metric = 'Renewable percentage'  # _debug_\n",
    "    title = f'{window_size.days}-day {title_metric} in [{start_date.strftime(\"%Y-%m-%d\")}, {end_date.strftime(\"%Y-%m-%d\")})'\n",
    "    global desired_renewable_ratio\n",
    "    if desired_renewable_ratio is not None:\n",
    "        title += f' ({desired_renewable_ratio*100}% renewable)'\n",
    "    if aggregate_by is not None:\n",
    "        title += f', grouped into {aggregate_by}'\n",
    "    plt.title(title)\n",
    "    # plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=4)\n",
    "    plt.xticks(rotation=15)\n",
    "    # plt.ylim(0, 400)\n",
    "    plt.tight_layout()\n",
    "    savefig_filename = 'carbon-intensity.timeseries.%s-%s.png' % (start_date.strftime(\"%Y%m%d\"), end_date.strftime(\"%Y%m%d\"))\n",
    "    if enable_savefig:\n",
    "        plt.savefig(savefig_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_carbon_intensity_overlap_cdf(all_region_time_series_data: dict[tuple, CarbonIntensityData], l_cloud_region_pairs: list[tuple[tuple, tuple]], start_date: datetime, end_date: datetime):\n",
    "    plt.figure()\n",
    "    # plt.figure(figsize=(5, 4))\n",
    "    for (cloud_region1, cloud_region2) in l_cloud_region_pairs:\n",
    "        carbon_data1 = all_region_time_series_data[cloud_region1]\n",
    "        carbon_data2 = all_region_time_series_data[cloud_region2]\n",
    "        region1_name = format_cloud_region_name(cloud_region1, carbon_data1.iso)\n",
    "        region2_name = format_cloud_region_name(cloud_region2, carbon_data2.iso)\n",
    "        overlap_intervals = find_overlap_interval_of_carbon_intensities(carbon_data1.timeseries,\n",
    "                                                                        carbon_data2.timeseries)\n",
    "        print(\"plotting CDF\")\n",
    "        plot_overlap_interval_cdf(overlap_intervals, f'{region1_name} < {region2_name}')\n",
    "        print(\"done\")\n",
    "    plt.xlabel('Overlap (hours)')\n",
    "    plt.ylabel('CDF')\n",
    "    plt.title('Carbon intensity overlap in [%s,%s)' % (start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2))\n",
    "    plt.tight_layout()\n",
    "    savefig_filename = 'carbon-intensity.overlap.%s-%s.png' % (start_date.strftime(\"%Y%m%d\"), end_date.strftime(\"%Y%m%d\"))\n",
    "    if enable_savefig:\n",
    "        plt.savefig(savefig_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_vendor_and_regions = []\n",
    "l_cloud_region_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# cloud_vendor_and_regions = list(map(lambda r: ('Azure', r), azure_regions_americas[0:2]))\n",
    "# cloud_vendor_and_regions = list(map(lambda r: ('Azure', r), azure_regions_americas))\n",
    "cloud_vendor_and_regions = list(map(lambda r: ('Azure', r), azure_regions_americas + azure_regions_europe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# cloud_vendor_and_regions = list(map(lambda r: ('Azure', r), azure_regions_americas))\n",
    "# cloud_vendor_and_regions = list(map(lambda r: ('Azure', r), azure_regions_europe))\n",
    "\n",
    "cloud_vendor_and_regions = list(map(lambda r: ('Azure', r), [\n",
    "    'centralus',\n",
    "    'westus',\n",
    "    # 'germanywestcentral', # too high\n",
    "    # 'francecentral',  # too low\n",
    "    'uksouth',\n",
    "    # 'northeurope',\n",
    "]))\n",
    "\n",
    "l_cloud_region_pairs = [\n",
    "    (('Azure', 'eastus'), ('Azure', 'uksouth')),\n",
    "    (('Azure', 'eastus'), ('Azure', 'westus')),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "utcnow = arrow.get(round_down(datetime.now(timezone.utc), timedelta(minutes=5))).shift(minutes=5)\n",
    "print(utcnow)\n",
    "all_prediction_data = pull_carbon_intensity_data(cloud_vendor_and_regions, utcnow, utcnow.shift(days=1), get_prediction=True)\n",
    "plot_carbon_intensity_time_series(all_prediction_data, utcnow.datetime, utcnow.shift(days=1).datetime, errorbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "cloud_vendor_and_regions = [\n",
    "    # ('AWS', 'us-west-1'),\n",
    "    # ('AWS', 'us-west-2'),\n",
    "    # ('AWS', 'us-east-1'),\n",
    "    # AWS us-east-2 uses the same ISO as us-east-1\n",
    "    # ('AWS', 'us-east-2'),\n",
    "\n",
    "    # This covers all c3lab regions\n",
    "    ('Azure', 'westus'),        # US-CAISO\n",
    "    ('BPA', 'eugene'),          # US-BPA\n",
    "    ('PACW', 'modford'),        # US-PACW\n",
    "    ('NY', 'syracuse'),         # US-NY\n",
    "    ('SPP', 'kansus'),          # US-SPP\n",
    "    ('Azure', 'southcentralus'),# US-ERCOT\n",
    "    ('Azure', 'eastus'),        # US-PJM\n",
    "    ('Azure', 'centralus'),     # US-MISO\n",
    "]\n",
    "\n",
    "# l_cloud_region_pairs = [\n",
    "#     (('AWS', 'us-east-1'), ('AWS', 'us-west-1')),\n",
    "#     (('AWS', 'us-east-1'), ('AWS', 'us-west-2'))\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = timedelta(days=365)\n",
    "# base_start_date = datetime.utcnow().date()\n",
    "base_start_date = datetime(2023, 1, 1, tzinfo=tz.UTC)\n",
    "desired_renewable_ratio = None\n",
    "d_region_time_series_data_by_offset = {}\n",
    "for offset in range(1):\n",
    "    start_date = arrow.get(base_start_date) + (window_size * -(1 + offset))\n",
    "    end_date = start_date + window_size\n",
    "    d_region_time_series_data_by_offset[(start_date, end_date)] = pull_carbon_intensity_data(cloud_vendor_and_regions, start_date, end_date, desired_renewable_ratio=desired_renewable_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single time range data, mean + errorbar\n",
    "\n",
    "for start_date, end_date in d_region_time_series_data_by_offset:\n",
    "    all_region_time_series_data = d_region_time_series_data_by_offset[(start_date, end_date)]\n",
    "    plot_carbon_intensity_time_series(all_region_time_series_data, start_date.shift(days=0).datetime, end_date.datetime, errorbar=True, aggregate_by=timedelta(days=1))\n",
    "    # break\n",
    "\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for continous time range\n",
    "l_timeranges_by_region = {}\n",
    "for start_date, end_date in d_region_time_series_data_by_offset:\n",
    "    # print(start_date, end_date)\n",
    "    all_region_time_series_data = d_region_time_series_data_by_offset[(start_date, end_date)]\n",
    "    for (cloud_vendor, region) in all_region_time_series_data:\n",
    "        # carbon_intensity_data = all_region_time_series_data[(cloud_vendor, region)]\n",
    "        if (cloud_vendor, region) not in l_timeranges_by_region:\n",
    "            l_timeranges_by_region[(cloud_vendor, region)] = []\n",
    "        l_timeranges_by_region[(cloud_vendor, region)].append((start_date, end_date))\n",
    "        # l.append(((cloud_vendor, region), (start_date, end_date)))\n",
    "for (cloud_vendor, region) in cloud_vendor_and_regions:\n",
    "    time_ranges = sorted(l_timeranges_by_region[(cloud_vendor, region)])\n",
    "    if len(time_ranges) == 0: continue\n",
    "    last_end = time_ranges[0][1]\n",
    "    for (start, end) in time_ranges[1:]:\n",
    "        if start != last_end:\n",
    "            raise ValueError(f\"Time range for {cloud_vendor}:{region} discontinuted between {last_end} and {start}\")\n",
    "        last_end = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge multiple time range data, mean (+ errorbar)\n",
    "\n",
    "all_region_merged_carbon_data = {}\n",
    "min_start_date = datetime.max.replace(tzinfo=tz.UTC)\n",
    "max_end_date = datetime.min.replace(tzinfo=tz.UTC)\n",
    "for (cloud_vendor, region) in cloud_vendor_and_regions:\n",
    "    print(cloud_vendor, region)\n",
    "    l_carbon_data = []\n",
    "    for start_date, end_date in d_region_time_series_data_by_offset:\n",
    "        l_carbon_data.append(d_region_time_series_data_by_offset[(start_date, end_date)][(cloud_vendor, region)])\n",
    "        min_start_date = min(min_start_date, start_date)\n",
    "        max_end_date = max(max_end_date, end_date)\n",
    "    # print(cloud_vendor, region, min_start_date, max_end_date, [str(x) for x in l_carbon_data])\n",
    "    all_region_merged_carbon_data[(cloud_vendor, region)] = CarbonIntensityData.merge(l_carbon_data)\n",
    "plot_carbon_intensity_time_series(all_region_merged_carbon_data, min_start_date.datetime, max_end_date.datetime, aggregate_by=timedelta(days=1), errorbar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Filter select regions for plotting\n",
    "\n",
    "# cloud_vendor_and_regions = list(map(lambda r: ('Azure', r), azure_regions_americas))\n",
    "cloud_vendor_and_regions = list(map(lambda r: ('Azure', r), azure_regions_europe))\n",
    "\n",
    "filtered_region_merged_carbon_data = {}\n",
    "min_start_date = datetime.max.replace(tzinfo=tz.UTC)\n",
    "max_end_date = datetime.min.replace(tzinfo=tz.UTC)\n",
    "for (cloud_vendor, region) in cloud_vendor_and_regions:\n",
    "    print(cloud_vendor, region)\n",
    "    carbon_intensity_data = all_region_merged_carbon_data[(cloud_vendor, region)]\n",
    "    min_start_date = min(min_start_date, carbon_intensity_data.timeseries_pd.index.min())\n",
    "    max_end_date = max(max_end_date, carbon_intensity_data.timeseries_pd.index.max())\n",
    "    filtered_region_merged_carbon_data[(cloud_vendor, region)] = carbon_intensity_data\n",
    "plot_carbon_intensity_time_series(filtered_region_merged_carbon_data, min_start_date, max_end_date, aggregate_by=timedelta(days=1), errorbar=True)\n",
    "plt.savefig('carbon_intensity.daily_average.azure.eu.errorbar.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, v = next(iter(d_region_time_series_data_by_offset.items()))\n",
    "ci_uswest1 = v[('AWS', 'us-west-1')]\n",
    "df = ci_uswest1.timeseries_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df:', type(df), len(df))\n",
    "# Wrong, cannot apply two conditions per element\n",
    "# df.loc[(pd.to_datetime(datetime(2022, 9, 30, 23, tzinfo=tz.UTC)) > df.index > pd.to_datetime(datetime(2022, 9, 30, 22, tzinfo=tz.UTC)))].index\n",
    "# Okay, but only one condition\n",
    "# df.loc[df.index > pd.to_datetime(datetime(2022, 9, 30, 22, tzinfo=tz.UTC)))].index\n",
    "filtered = df.loc[datetime(2022, 9, 30, 22, tzinfo=tz.UTC):datetime(2022, 9, 30, 23, tzinfo=tz.UTC)]\n",
    "print('filtered: ', type(filtered), len(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.core.groupby.DataFrameGroupBy.agg.html\n",
    "\n",
    "series_name = \"AWS:us-west-1\"\n",
    "\n",
    "df = filtered.to_frame(series_name)\n",
    "dt = timedelta(minutes=30)\n",
    "df_agg = df.assign(time=lambda x: (x.index.to_pydatetime() - datetime.min.replace(tzinfo=tz.UTC)) % dt).groupby('time').agg(['min', 'max', 'mean', 'std'])\n",
    "print(df_agg)\n",
    "df_agg[series_name]['min'].plot()\n",
    "plt.ylim(0, 200)\n",
    "plt.legend()\n",
    "# df_agg.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Concatenate two series with different index range\n",
    "\n",
    "# Example:\n",
    "s1 = pd.Series({datetime(2022, 1, 1, 0): 0, datetime(2022, 1, 1, 1): 1})\n",
    "s2 = pd.Series({datetime(2022, 1, 1, 2): 2, datetime(2022, 1, 1, 3): 3})\n",
    "combined = pd.concat([s2, s1]).sort_index()\n",
    "dedupped = combined[~combined.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "for start_date, end_date in d_region_time_series_data_by_offset:\n",
    "    all_region_time_series_data = d_region_time_series_data_by_offset[(start_date, end_date)]\n",
    "    plot_carbon_intensity_overlap_cdf(all_region_time_series_data, l_cloud_region_pairs, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diff_carbon_intensity_data(ci1: CarbonIntensityData, ci2: CarbonIntensityData, diff_timeseries: list[TimestampdValue], diff_ds: pd.Series) -> CarbonIntensityData:\n",
    "    diff_region_name = f'({ci1.cloud_vendor}:{ci1.region} - {ci2.cloud_vendor}:{ci2.region})'\n",
    "    diff_iso_name = f'{ci1.iso} - {ci2.iso}'\n",
    "    return CarbonIntensityData('diff', diff_region_name, diff_iso_name, diff_timeseries, diff_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_carbon_intensity(ci1: CarbonIntensityData, ci2: CarbonIntensityData) -> CarbonIntensityData:\n",
    "    diff_timeseries = []\n",
    "    ds1 = ci1.timeseries_pd\n",
    "    ds2 = ci2.timeseries_pd\n",
    "    combined_index = sorted(set(ds1.index.tolist() + ds2.index.tolist()))\n",
    "    ds1 = ds1.reindex(combined_index, method='ffill')\n",
    "    ds2 = ds2.reindex(combined_index, method='ffill')\n",
    "    diff_ds = ds1 - ds2\n",
    "    diff_ds.dropna()\n",
    "    diff_timeseries = CarbonIntensityData.create_timeseries_from_pd(diff_ds)\n",
    "    diff_region_name = f'({ci1.cloud_vendor}:{ci1.region} - {ci2.cloud_vendor}:{ci2.region})'\n",
    "    diff_iso_name = f'{ci1.iso} - {ci2.iso}'\n",
    "    return create_diff_carbon_intensity_data(ci1, ci2, diff_timeseries, diff_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for start_date, end_date in d_region_time_series_data_by_offset:\n",
    "    all_region_time_series_data = d_region_time_series_data_by_offset[(start_date, end_date)]\n",
    "    diff_time_series_data = {}\n",
    "    for cr1, cr2 in l_cloud_region_pairs:\n",
    "        ci_data_diff = get_diff_carbon_intensity(\n",
    "            all_region_time_series_data[cr1],\n",
    "            all_region_time_series_data[cr2]\n",
    "        )\n",
    "        diff_time_series_data[(ci_data_diff.cloud_vendor, ci_data_diff.region)] = ci_data_diff\n",
    "    plot_end_date = start_date + timedelta(days=1)\n",
    "    if diff_time_series_data:\n",
    "        # plot_carbon_intensity_time_series(all_region_time_series_data, start_date, plot_end_date)\n",
    "        plot_carbon_intensity_time_series(diff_time_series_data, start_date, plot_end_date)\n",
    "        plt.axhline(y=0, color='k')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_region_time_series_data_by_offset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c686f18b94c7ac93143553ca22fc47dddb45e62f4fa825973de80c268de3ec5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
